{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两种集成方法\n",
    "\n",
    "# 在bagging中，是通过随机抽样的替换方式，得到了与原始数据集规模一样的数据集；\n",
    "# 而Boosting在Bagging的思路上更进一步，在数据集上顺序应用了多个不同的分类器。 \n",
    "  另一个成功的集成方法时随机森林，但是不如AdaBoost流行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类算法：k近邻，决策树，朴素贝叶斯，logistic回归，支持向量机\n",
    "\n",
    "### 7.1 基于数据集多重抽样的分类器\n",
    "对于之前提到的5种分类算法，将不同的分类器组合起来，而这种组合结果则被称为集成方法或者元算法。\n",
    "使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。\n",
    "接下来介绍一种基于同一种分类器多个不同实例的两种计算方法。在这些方法当中，数据集也会不断变化，而后应用于不同的实例分类器上。最后讨论如何利用机器学习问题的通用框架来应用AdaBoost算法。\n",
    "\n",
    "AdaBoost\n",
    "优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。\n",
    "缺点：对离群点敏感\n",
    "适用数据类型：数值型和标称型数据\n",
    "\n",
    "\n",
    "### 7.1.1 bagging 基于数据随机重抽样的分类器构建方法\n",
    "#### bagging方法，也称为自举汇聚法，是从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原数据集的大小相等。\n",
    "\n",
    "#### boosting\n",
    "在boosting中，不同的分类器是通过串行训练得到的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。\n",
    "\n",
    "\n",
    "### 7.2 训练算法：基于错误提升分类器的性能\n",
    "AdaBoosting是adaptive boosting（自适应boosting）的缩写，其运行过程如下：训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量D.一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。\n",
    "\n",
    "\n",
    "### 7.3 基于单层决策树构建弱分类器\n",
    "单层决策树（也称决策树桩）是一种简单的决策树。仅基于单个特征来做决策，由于这棵树只有一次分裂过程，因此实际上就是一个树桩。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSimpData():\n",
    "    datMat = ([[1.,2.1],\n",
    "                    [2.,1.1],\n",
    "                    [1.3,1.],\n",
    "                    [1.,1.],\n",
    "                    [2.,1.]])\n",
    "    classLabels = [1.0,1.0,-1.0,-1.0,1.0]\n",
    "    return datMat,classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.1], [2.0, 1.1], [1.3, 1.0], [1.0, 1.0], [2.0, 1.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datMat,classLabels = loadSimpData()\n",
    "datMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了数据，接下来就可以通过构建多个函数来建立单层决策树\n",
    "第一个函数将用于测试是否有某个小于或者大于我们正在测试的阈值。第二个函数复杂一下，它会在一个加权数据集中循环，并找到具有最低错误率的单层决策树。\n",
    "\n",
    "将最小错误率minError设为+无穷\n",
    "对数据集中的每一个特征（第一层循环）：\n",
    "    对每个步长（第二层循环）：\n",
    "        对每个不等号（第三层循环）：\n",
    "            建立一棵单层决策树并利用加权数据集对它进行测试\n",
    "            如果错误率低于minError，则将当前单层决策树设为最佳单层决策树\n",
    " 返回最佳单层决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "训练弱分类器的过程就是从已有的特征中选出一个特征以及其对应的阈值，使样本分错的错误率最低，即寻找一个最小分错率的过程。\n",
    "\n",
    "最小错误率初始化为无穷大；\n",
    "遍历样本的所有特征（本例子每个样本有三个特征，即遍历这三个特征值）；\n",
    "求出该特征值步长（不同特征不一样），（最大特征值-最小特征值）/步长移动次数，如本例，假设步长移动次数为10，则第一个特征步长为（7-1）/10 = 0.6；\n",
    "根据特征值步长开始从最小特征值遍历到最大特征值；\n",
    "遍历判断符号，大于还是小于；\n",
    "计算出阈值（根据最小特征值及步长），根据阈值、符号、及特征索引、开始对样本分类；\n",
    "根据每个样本权重以及分类结果计算分错率，若该分错率小于最小分错率，则更新最小分错率；\n",
    "返回最小分错率下的特征索引、符号、阈值，即得到弱分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#单层决策树生成函数\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#通过阈值比较对数据进行分类\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))#首先将返回数组的全部元素设置为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr,classLabels,D):#遍历stumpClassify函数所有的可能输入值\n",
    "    dataMatrix = np.mat(dataArr);labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps =  10.0;bestStump = {}#构建一个字典\n",
    "    bestClassEst = np.mat(np.zeros((m,1)))\n",
    "    minError = np.inf#最小错误率初始化为无穷大\n",
    "    for i in range(n):#遍历样本的所有特征\n",
    "        rangeMin = dataMatrix[:,i].min()\n",
    "        rangeMax = dataMatrix[:,i].max()\n",
    "        stepSize = (rangeMax - rangeMin)/numSteps#求出该特征值的步长（不同特征不一样）\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                errArr = np.mat(np.ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T*errArr\n",
    "                #print(\"split: dim %d,thresh %.2f, thresh inequal: %s,the weighted error is %.3f\" %(i,threshVal,inequal,weightedError))\n",
    "                if weightedError < minError:#将当前的错误率和已有的最小错误率对比\n",
    "                    minError = weightedError#如果当前值较小，就在词典bestStump中保存该单层决策树\n",
    "                    bestClassEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i#字典，错误率和类别估计都会返回给AdaBoost算法\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClassEst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.2],\n",
       "        [0.2],\n",
       "        [0.2],\n",
       "        [0.2],\n",
       "        [0.2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D= np.mat(np.ones((5,1))/5)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0,thresh 0.90, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 0,thresh 0.90, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 0,thresh 1.00, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 0,thresh 1.00, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 0,thresh 1.10, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 0,thresh 1.10, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 0,thresh 1.20, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 0,thresh 1.20, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 0,thresh 1.30, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.30, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.40, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.40, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.50, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.50, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.60, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.60, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.70, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.70, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.80, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.80, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 1.90, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 0,thresh 1.90, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 0,thresh 2.00, thresh inequal: lt,the weighted error is 0.600\n",
      "split: dim 0,thresh 2.00, thresh inequal: gt,the weighted error is 0.400\n",
      "split: dim 1,thresh 0.89, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 0.89, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.00, thresh inequal: lt,the weighted error is 0.200\n",
      "split: dim 1,thresh 1.00, thresh inequal: gt,the weighted error is 0.800\n",
      "split: dim 1,thresh 1.11, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.11, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.22, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.22, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.33, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.33, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.44, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.44, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.55, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.55, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.66, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.66, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.77, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.77, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.88, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.88, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 1.99, thresh inequal: lt,the weighted error is 0.400\n",
      "split: dim 1,thresh 1.99, thresh inequal: gt,the weighted error is 0.600\n",
      "split: dim 1,thresh 2.10, thresh inequal: lt,the weighted error is 0.600\n",
      "split: dim 1,thresh 2.10, thresh inequal: gt,the weighted error is 0.400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},\n",
       " matrix([[0.2]]),\n",
       " array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildStump(datMat,classLabels,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每次迭代：\n",
    "    利用buildStump()函数找到最佳的单层决策树\n",
    "    将最佳单层决策树加入到单层决策树数组\n",
    "    计算alpha\n",
    "    计算新的权重向量D\n",
    "    更新累计类别估计值\n",
    "    如果错误率等于0.0，则退出循环    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算法的输入参数包括数据集、类别标签以及迭代次数numIt，其中numIt是在整个AdaBoost算法中唯一需要用户指定的参数\n",
    "def adaboostTrainDS(dataArr,classLabels,numIt=40):#函数名称尾部的DS代表单层决策树\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    #D是一个概率分布向量，因此所有的元素之和为1.所以一开始所有的元素初始化为1/m\n",
    "    D = np.mat(np.ones((m,1))/m)#建立一个列向量，包含了每个数据点的权重，概率分布向量\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))#列向量 用于记录每个数据点的类别估计累计值\n",
    "    for i in range(numIt):#核心部分\n",
    "        #建立一个单层决策树，输入为D，返回利用D得到的具有最小错误率的单层决策树，最小错误率以及估计的类别向量\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)\n",
    "        print(\"D:\", D.T)\n",
    "        alpha = np.float(0.5*np.log((1.0-error)/max(error,1e-16)))#alpha用于告诉总分类器本次DS输出结果的权重\n",
    "        bestStump['alpha'] = alpha#加入字典中\n",
    "        weakClassArr.append(bestStump)#字典添加到列表，字典包含了分类所需要的所有信息\n",
    "        print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1*alpha*np.mat(classLabels).T,classEst)#计算下一次迭代的新权重\n",
    "        D = np.multiply(D,np.exp(expon))\n",
    "        D = D/D.sum()\n",
    "        aggClassEst += alpha*classEst\n",
    "        print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T,np.ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2\n",
      "D: [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2\n",
      "D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifierArray = adaboostTrainDS(datMat,classLabels,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数组包含了三部词典，其中包含了分类所需要的所有信息。此时一个分类器已经构建成功，而且只要我们愿意随时都可以将训练错误率降到0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 测试算法：基于AdaBoost的分类\n",
    "\n",
    "一旦拥有多个弱分类器以及其对应的alpha值，进行测试就变得相当容易了。在AdaboostTrainDS中已经写完了大部分的代码，现在需要做的就只是将弱分类器的训练过程从程序中抽出来，然后应用到某个具体的实例上去。每个弱分类器的结果以其对应的alpha值作为权重。所有这些弱分类器的结果加权求和就得到了最后的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#利用训练出的多个弱分类器进行分类的函数\n",
    "该函数的输入是由一个或者多个待分类样例datToClass以及多个弱分类器组成的数据classifierArr。\n",
    "函数adaClassify()首先将datToClass转换成一个Numpy矩阵，并且得到datToClass中待分类样例的个数m\n",
    "\"\"\"\n",
    "def adaClassify(datToClass,classifierArr):\n",
    "    dataMatrix = np.mat(datToClass)\n",
    "    m = np.shape(dataMatrix)[0]\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))#构建一个0列向量 用于记录每个数据点的类别估计累计值\n",
    "    for i in range(len(classifierArr)):#遍历classifierArr中的所有弱分类器，基于stumpClassify对每个分类器得到一个类别的估计值\n",
    "        classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],\n",
    "                                 classifierArr[i]['thresh'],classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst#类别估计值乘上该DS的alpha权重然后累加到aggClassEst上\n",
    "        print(aggClassEst)\n",
    "    return np.sign(aggClassEst)#返回aggClassEst的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2\n",
      "D: [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2\n",
      "D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0\n",
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datArr,labelArr = loadSimpData() \n",
    "classifierArr = adaboostTrainDS(datArr,labelArr,30)\n",
    "adaClassify([0,0],classifierArr)\n",
    "\"\"\"可以发现，随着迭代的进行，数据点[0,0]的分类结果越来越强\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69314718]\n",
      " [-0.69314718]]\n",
      "[[ 1.66610226]\n",
      " [-1.66610226]]\n",
      "[[ 2.56198199]\n",
      " [-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.],\n",
       "        [-1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"也可以在其他点上进行分类\"\"\"\n",
    "adaClassify([[5,5],[0,0]],classifierArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 示例：在一个难数据集上应用AdaBoost\n",
    "\n",
    "* 收集数据：提供的文本文件\n",
    "* 准备数据：确保类别标签是+1和-1而非1和0\n",
    "* 分析数据：手工检查数据\n",
    "* 训练算法：在数据上，利用adaboostTrainDS（）函数训练出一系列的分类器\n",
    "* 测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，对AdaBoost和Logistic回归的结果进行完全对等的比较\n",
    "* 使用算法：观察该例子上的错误率。也可以构建一个Web网站，输入马的症状然后预测马是否会死去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自适应数据加载函数\n",
    "\"\"\"在这里并不指定每个文件中的特征数目，\n",
    "该函数能够自动检测出特征的数目。同时，该函数也假定最后一个特征是类别标签\"\"\"\n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t'))\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(np.float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(np.float(curLine[-1]))\n",
    "    return dataMat,labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448 0.00334448\n",
      "  0.00334448 0.00334448 0.00334448 0.00334448 0.00334448]]\n",
      "classEst:  [[-1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "aggClassEst:  [[-0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238 -0.46166238  0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238 -0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238 -0.46166238 -0.46166238 -0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238 -0.46166238 -0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238 -0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238 -0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238 -0.46166238 -0.46166238 -0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238 -0.46166238\n",
      "  -0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238\n",
      "   0.46166238 -0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238  0.46166238  0.46166238  0.46166238 -0.46166238 -0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238 -0.46166238 -0.46166238\n",
      "  -0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238 -0.46166238 -0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "  -0.46166238 -0.46166238 -0.46166238  0.46166238 -0.46166238 -0.46166238\n",
      "   0.46166238  0.46166238  0.46166238  0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238  0.46166238  0.46166238 -0.46166238  0.46166238  0.46166238\n",
      "   0.46166238 -0.46166238 -0.46166238  0.46166238  0.46166238]]\n",
      "total error:  0.2842809364548495\n",
      "D: [[0.00233645 0.00588235 0.00233645 0.00588235 0.00588235 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00588235 0.00588235 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00588235 0.00233645 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00588235 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00588235 0.00233645 0.00588235 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00588235 0.00233645 0.00233645 0.00588235\n",
      "  0.00588235 0.00233645 0.00233645 0.00588235 0.00588235 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00588235 0.00588235 0.00588235 0.00233645 0.00588235 0.00588235\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00233645 0.00588235\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00588235 0.00233645 0.00233645 0.00588235 0.00588235 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00588235 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00588235 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00233645 0.00588235\n",
      "  0.00233645 0.00588235 0.00588235 0.00588235 0.00233645 0.00588235\n",
      "  0.00588235 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00588235\n",
      "  0.00233645 0.00588235 0.00233645 0.00588235 0.00588235 0.00588235\n",
      "  0.00588235 0.00588235 0.00233645 0.00588235 0.00588235 0.00233645\n",
      "  0.00588235 0.00588235 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00588235 0.00233645 0.00233645 0.00233645 0.00233645 0.00588235\n",
      "  0.00588235 0.00588235 0.00233645 0.00588235 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00588235 0.00588235\n",
      "  0.00233645 0.00233645 0.00233645 0.00233645 0.00233645 0.00233645\n",
      "  0.00233645 0.00233645 0.00588235 0.00233645 0.00588235 0.00233645\n",
      "  0.00588235 0.00233645 0.00233645 0.00233645 0.00588235]]\n",
      "classEst:  [[ 1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.\n",
      "   1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.]]\n",
      "aggClassEst:  [[-0.14917993  0.77414483  0.77414483  0.77414483  0.14917993  0.77414483\n",
      "   0.77414483  0.77414483 -0.14917993 -0.14917993  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483 -0.14917993 -0.77414483  0.77414483\n",
      "   0.77414483  0.14917993 -0.14917993  0.77414483  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "  -0.77414483  0.77414483  0.77414483  0.77414483 -0.14917993 -0.77414483\n",
      "   0.77414483 -0.77414483  0.77414483  0.77414483 -0.77414483  0.77414483\n",
      "   0.77414483 -0.14917993 -0.77414483 -0.77414483 -0.77414483  0.77414483\n",
      "   0.14917993 -0.77414483  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "   0.14917993  0.77414483  0.77414483 -0.14917993 -0.77414483 -0.77414483\n",
      "   0.77414483  0.77414483  0.14917993  0.14917993  0.77414483  0.77414483\n",
      "   0.14917993  0.77414483  0.14917993  0.77414483  0.77414483  0.77414483\n",
      "   0.77414483 -0.77414483  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "  -0.14917993  0.14917993 -0.14917993  0.77414483  0.77414483 -0.77414483\n",
      "   0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483  0.77414483\n",
      "   0.14917993  0.14917993  0.77414483 -0.14917993  0.77414483  0.77414483\n",
      "   0.77414483  0.14917993  0.77414483  0.14917993  0.77414483 -0.14917993\n",
      "   0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483  0.77414483\n",
      "  -0.77414483  0.77414483  0.14917993  0.77414483  0.77414483  0.77414483\n",
      "  -0.14917993  0.77414483  0.77414483  0.77414483 -0.14917993  0.77414483\n",
      "   0.77414483 -0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483\n",
      "  -0.14917993 -0.14917993  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "   0.77414483 -0.77414483 -0.14917993 -0.14917993  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.14917993 -0.77414483  0.77414483  0.14917993\n",
      "   0.77414483  0.77414483  0.77414483  0.14917993  0.77414483  0.77414483\n",
      "  -0.77414483  0.14917993  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483\n",
      "  -0.14917993  0.77414483  0.14917993  0.77414483  0.77414483  0.14917993\n",
      "  -0.14917993  0.77414483  0.14917993  0.14917993  0.14917993 -0.14917993\n",
      "   0.77414483  0.77414483 -0.14917993 -0.14917993 -0.14917993 -0.14917993\n",
      "   0.77414483 -0.77414483  0.77414483  0.77414483  0.14917993  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483  0.77414483 -0.14917993  0.77414483\n",
      "  -0.77414483  0.77414483  0.77414483  0.14917993  0.14917993  0.77414483\n",
      "   0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483 -0.14917993\n",
      "  -0.14917993 -0.14917993  0.77414483  0.14917993  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "   0.14917993  0.77414483  0.77414483 -0.14917993  0.77414483  0.14917993\n",
      "   0.77414483  0.77414483  0.77414483  0.14917993  0.77414483 -0.77414483\n",
      "   0.77414483 -0.14917993 -0.77414483  0.77414483  0.77414483  0.77414483\n",
      "  -0.77414483  0.77414483  0.77414483  0.77414483 -0.14917993 -0.77414483\n",
      "   0.77414483 -0.14917993  0.77414483  0.77414483 -0.77414483 -0.14917993\n",
      "  -0.14917993 -0.14917993  0.77414483  0.14917993  0.77414483  0.77414483\n",
      "  -0.14917993 -0.14917993 -0.14917993  0.77414483  0.14917993  0.77414483\n",
      "   0.14917993 -0.14917993  0.77414483  0.77414483 -0.14917993  0.77414483\n",
      "   0.77414483  0.77414483  0.77414483  0.77414483  0.77414483  0.77414483\n",
      "  -0.14917993 -0.14917993 -0.77414483  0.77414483 -0.14917993 -0.14917993\n",
      "   0.77414483  0.77414483  0.77414483  0.77414483  0.14917993  0.14917993\n",
      "   0.77414483  0.77414483  0.77414483 -0.14917993  0.77414483  0.77414483\n",
      "   0.77414483  0.77414483  0.14917993 -0.14917993  0.77414483  0.77414483\n",
      "   0.14917993 -0.14917993 -0.77414483  0.77414483  0.77414483]]\n",
      "total error:  0.2842809364548495\n",
      "D: [[0.00335068 0.00843582 0.00179355 0.00843582 0.00451553 0.00179355\n",
      "  0.00179355 0.00843582 0.00335068 0.00451553 0.00179355 0.00179355\n",
      "  0.00179355 0.00843582 0.00179355 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00335068 0.00451553 0.00179355 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00179355 0.00179355 0.00843582 0.00179355\n",
      "  0.00179355 0.00843582 0.00179355 0.00179355 0.00451553 0.00179355\n",
      "  0.00843582 0.00843582 0.00179355 0.00843582 0.00179355 0.00179355\n",
      "  0.00179355 0.00335068 0.00179355 0.00179355 0.00843582 0.00179355\n",
      "  0.00451553 0.00179355 0.00179355 0.00843582 0.00179355 0.00179355\n",
      "  0.00451553 0.00843582 0.00179355 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00451553 0.00451553 0.00179355 0.00179355\n",
      "  0.00335068 0.00179355 0.00335068 0.00179355 0.00843582 0.00179355\n",
      "  0.00843582 0.00179355 0.00843582 0.00843582 0.00179355 0.00179355\n",
      "  0.00335068 0.00335068 0.00335068 0.00179355 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00335068 0.00843582 0.00179355 0.00179355\n",
      "  0.00335068 0.00451553 0.00179355 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00451553 0.00179355 0.00335068 0.00179355 0.00335068\n",
      "  0.00179355 0.00179355 0.00335068 0.00179355 0.00179355 0.00179355\n",
      "  0.00179355 0.00843582 0.00451553 0.00179355 0.00179355 0.00843582\n",
      "  0.00451553 0.00179355 0.00179355 0.00843582 0.00451553 0.00179355\n",
      "  0.00179355 0.00179355 0.00179355 0.00335068 0.00843582 0.00179355\n",
      "  0.00451553 0.00451553 0.00843582 0.00179355 0.00843582 0.00843582\n",
      "  0.00179355 0.00179355 0.00335068 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00451553 0.00179355 0.00179355 0.00451553\n",
      "  0.00179355 0.00179355 0.00843582 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00335068 0.00179355 0.00179355 0.00179355 0.00179355\n",
      "  0.00843582 0.00179355 0.00179355 0.00451553 0.00843582 0.00179355\n",
      "  0.00335068 0.00179355 0.00451553 0.00179355 0.00179355 0.00335068\n",
      "  0.00335068 0.00179355 0.00451553 0.00335068 0.00451553 0.00335068\n",
      "  0.00179355 0.00179355 0.00335068 0.00451553 0.00335068 0.00335068\n",
      "  0.00179355 0.00179355 0.00179355 0.00179355 0.00335068 0.00179355\n",
      "  0.00179355 0.00179355 0.00179355 0.00179355 0.00451553 0.00179355\n",
      "  0.00179355 0.00179355 0.00179355 0.00451553 0.00451553 0.00179355\n",
      "  0.00179355 0.00179355 0.00335068 0.00843582 0.00179355 0.00451553\n",
      "  0.00335068 0.00451553 0.00843582 0.00451553 0.00179355 0.00843582\n",
      "  0.00843582 0.00179355 0.00179355 0.00179355 0.00179355 0.00179355\n",
      "  0.00335068 0.00843582 0.00179355 0.00335068 0.00179355 0.00335068\n",
      "  0.00179355 0.00179355 0.00179355 0.00451553 0.00179355 0.00179355\n",
      "  0.00179355 0.00451553 0.00179355 0.00179355 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00179355 0.00843582 0.00335068 0.00179355\n",
      "  0.00179355 0.00335068 0.00179355 0.00179355 0.00179355 0.00451553\n",
      "  0.00335068 0.00451553 0.00179355 0.00451553 0.00843582 0.00843582\n",
      "  0.00451553 0.00451553 0.00335068 0.00843582 0.00451553 0.00179355\n",
      "  0.00451553 0.00451553 0.00179355 0.00179355 0.00335068 0.00179355\n",
      "  0.00843582 0.00179355 0.00179355 0.00179355 0.00179355 0.00843582\n",
      "  0.00451553 0.00451553 0.00179355 0.00843582 0.00335068 0.00335068\n",
      "  0.00179355 0.00179355 0.00179355 0.00179355 0.00451553 0.00451553\n",
      "  0.00179355 0.00179355 0.00179355 0.00335068 0.00179355 0.00179355\n",
      "  0.00179355 0.00179355 0.00451553 0.00335068 0.00843582 0.00179355\n",
      "  0.00451553 0.00335068 0.00179355 0.00179355 0.00843582]]\n",
      "classEst:  [[-1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1.\n",
      "  -1. -1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  -1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1.\n",
      "   1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.\n",
      "  -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  -1. -1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
      "   1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1.\n",
      "  -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  -1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  -1. -1. -1. -1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.\n",
      "   1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.43598966  0.4873351   1.06095456  0.4873351  -0.1376298   1.06095456\n",
      "   1.06095456  0.4873351  -0.43598966 -0.43598966  0.4873351   0.4873351\n",
      "   1.06095456  0.4873351   0.4873351  -0.43598966 -1.06095456  1.06095456\n",
      "   0.4873351  -0.1376298  -0.43598966  1.06095456  1.06095456  0.4873351\n",
      "   0.4873351   0.4873351   0.4873351   1.06095456  1.06095456  1.06095456\n",
      "  -1.06095456  0.4873351   0.4873351   0.4873351  -0.43598966 -1.06095456\n",
      "   0.4873351  -1.06095456  1.06095456  0.4873351  -1.06095456  0.4873351\n",
      "   0.4873351  -0.43598966 -1.06095456 -1.06095456 -1.06095456  1.06095456\n",
      "  -0.1376298  -1.06095456  1.06095456  0.4873351   1.06095456  1.06095456\n",
      "  -0.1376298   0.4873351   1.06095456 -0.43598966 -0.4873351  -1.06095456\n",
      "   1.06095456  0.4873351  -0.1376298  -0.1376298   0.4873351   0.4873351\n",
      "  -0.1376298   1.06095456  0.43598966  1.06095456  0.4873351   1.06095456\n",
      "   0.4873351  -0.4873351   1.06095456  0.4873351   1.06095456  1.06095456\n",
      "   0.1376298  -0.1376298  -0.43598966  1.06095456  0.4873351  -0.4873351\n",
      "   0.4873351   1.06095456 -0.43598966  1.06095456  0.4873351   1.06095456\n",
      "   0.43598966 -0.1376298   1.06095456  0.1376298   1.06095456  1.06095456\n",
      "   0.4873351  -0.1376298   1.06095456 -0.1376298   1.06095456 -0.43598966\n",
      "   1.06095456  0.4873351   0.1376298   0.4873351   1.06095456  1.06095456\n",
      "  -1.06095456  0.4873351  -0.1376298   1.06095456  1.06095456  0.4873351\n",
      "   0.1376298   1.06095456  1.06095456  1.06095456 -0.43598966  0.4873351\n",
      "   1.06095456 -1.06095456  1.06095456 -0.43598966  1.06095456  1.06095456\n",
      "   0.1376298   0.1376298   0.4873351   1.06095456  1.06095456  0.4873351\n",
      "   1.06095456 -1.06095456 -0.43598966  0.1376298   1.06095456  0.4873351\n",
      "   0.4873351   1.06095456  0.43598966 -1.06095456  1.06095456 -0.1376298\n",
      "   0.4873351   1.06095456  1.06095456 -0.1376298   1.06095456  1.06095456\n",
      "  -0.4873351  -0.1376298   1.06095456  1.06095456  1.06095456  1.06095456\n",
      "   0.4873351   1.06095456  1.06095456  0.1376298   1.06095456  0.4873351\n",
      "  -0.43598966  0.4873351  -0.1376298   1.06095456  1.06095456 -0.1376298\n",
      "  -0.43598966  1.06095456 -0.1376298  -0.1376298  -0.1376298   0.1376298\n",
      "   1.06095456  0.4873351  -0.43598966 -0.43598966 -0.43598966 -0.43598966\n",
      "   1.06095456 -0.4873351   0.4873351   0.4873351  -0.1376298   0.4873351\n",
      "   0.4873351   0.4873351   0.4873351   1.06095456 -0.43598966  1.06095456\n",
      "  -1.06095456  0.4873351   1.06095456 -0.1376298  -0.1376298   0.4873351\n",
      "   0.4873351   1.06095456 -0.43598966  1.06095456  1.06095456  0.1376298\n",
      "  -0.43598966 -0.43598966  1.06095456 -0.1376298   1.06095456  0.4873351\n",
      "   0.4873351   0.4873351   1.06095456  0.4873351   1.06095456  1.06095456\n",
      "  -0.1376298   1.06095456  1.06095456 -0.43598966  0.4873351  -0.1376298\n",
      "   0.4873351   1.06095456  1.06095456 -0.1376298   1.06095456 -1.06095456\n",
      "   0.4873351  -0.43598966 -1.06095456  1.06095456  0.4873351   1.06095456\n",
      "  -0.4873351   1.06095456  1.06095456  0.4873351  -0.43598966 -1.06095456\n",
      "   1.06095456  0.1376298   0.4873351   0.4873351  -1.06095456 -0.43598966\n",
      "  -0.43598966 -0.43598966  1.06095456 -0.1376298   1.06095456  0.4873351\n",
      "  -0.43598966 -0.43598966 -0.43598966  0.4873351  -0.1376298   1.06095456\n",
      "  -0.1376298   0.1376298   1.06095456  0.4873351  -0.43598966  1.06095456\n",
      "   0.4873351   0.4873351   1.06095456  1.06095456  0.4873351   0.4873351\n",
      "  -0.43598966 -0.43598966 -1.06095456  0.4873351  -0.43598966  0.1376298\n",
      "   1.06095456  1.06095456  0.4873351   1.06095456 -0.1376298  -0.1376298\n",
      "   0.4873351   1.06095456  0.4873351  -0.43598966  0.4873351   1.06095456\n",
      "   1.06095456  1.06095456 -0.1376298  -0.43598966  0.4873351   1.06095456\n",
      "  -0.1376298  -0.43598966 -1.06095456  0.4873351   1.06095456]]\n",
      "total error:  0.24749163879598662\n",
      "D: [[0.00261936 0.00659463 0.00140209 0.00659463 0.00352998 0.00140209\n",
      "  0.00140209 0.00659463 0.00261936 0.00626457 0.00248826 0.00248826\n",
      "  0.00140209 0.00659463 0.00248826 0.00261936 0.00140209 0.00140209\n",
      "  0.00248826 0.00464853 0.00626457 0.00140209 0.00140209 0.00248826\n",
      "  0.00248826 0.00248826 0.00248826 0.00140209 0.01170335 0.00140209\n",
      "  0.00140209 0.00659463 0.00248826 0.00248826 0.00626457 0.00140209\n",
      "  0.00659463 0.01170335 0.00140209 0.00659463 0.00140209 0.00248826\n",
      "  0.00248826 0.00261936 0.00140209 0.00140209 0.01170335 0.00140209\n",
      "  0.00352998 0.00140209 0.00140209 0.00659463 0.00140209 0.00140209\n",
      "  0.00352998 0.00659463 0.00140209 0.00261936 0.00248826 0.00140209\n",
      "  0.00140209 0.00248826 0.00352998 0.00352998 0.00248826 0.00248826\n",
      "  0.00464853 0.00140209 0.00261936 0.00140209 0.00659463 0.00140209\n",
      "  0.00659463 0.00248826 0.01170335 0.00659463 0.00140209 0.00140209\n",
      "  0.00464853 0.00464853 0.00261936 0.00140209 0.00248826 0.00248826\n",
      "  0.00248826 0.00140209 0.00261936 0.01170335 0.00248826 0.00140209\n",
      "  0.00261936 0.00352998 0.00140209 0.00464853 0.00140209 0.00140209\n",
      "  0.00248826 0.00352998 0.00140209 0.00464853 0.00140209 0.00261936\n",
      "  0.00140209 0.00248826 0.00464853 0.00248826 0.00140209 0.00140209\n",
      "  0.00140209 0.00659463 0.00352998 0.00140209 0.00140209 0.00659463\n",
      "  0.00352998 0.00140209 0.00140209 0.01170335 0.00626457 0.00248826\n",
      "  0.00140209 0.00140209 0.00140209 0.00261936 0.01170335 0.00140209\n",
      "  0.00352998 0.00352998 0.00659463 0.00140209 0.01170335 0.00659463\n",
      "  0.00140209 0.00140209 0.00261936 0.00464853 0.00140209 0.00248826\n",
      "  0.00248826 0.00140209 0.00626457 0.00140209 0.00140209 0.00352998\n",
      "  0.00248826 0.00140209 0.01170335 0.00464853 0.00140209 0.00140209\n",
      "  0.00248826 0.00464853 0.00140209 0.00140209 0.00140209 0.00140209\n",
      "  0.00659463 0.00140209 0.00140209 0.00352998 0.01170335 0.00248826\n",
      "  0.00261936 0.00248826 0.00352998 0.00140209 0.00140209 0.00464853\n",
      "  0.00261936 0.00140209 0.00352998 0.00464853 0.00352998 0.00464853\n",
      "  0.00140209 0.00248826 0.00261936 0.00626457 0.00261936 0.00261936\n",
      "  0.00140209 0.00248826 0.00248826 0.00248826 0.00464853 0.00248826\n",
      "  0.00248826 0.00248826 0.00248826 0.00140209 0.00626457 0.00140209\n",
      "  0.00140209 0.00248826 0.00140209 0.00352998 0.00352998 0.00248826\n",
      "  0.00248826 0.00140209 0.00261936 0.01170335 0.00140209 0.00352998\n",
      "  0.00261936 0.00626457 0.01170335 0.00352998 0.00140209 0.00659463\n",
      "  0.00659463 0.00248826 0.00140209 0.00248826 0.00140209 0.00140209\n",
      "  0.00464853 0.01170335 0.00140209 0.00261936 0.00248826 0.00464853\n",
      "  0.00248826 0.00140209 0.00140209 0.00352998 0.00140209 0.00140209\n",
      "  0.00248826 0.00626457 0.00140209 0.00140209 0.00248826 0.00140209\n",
      "  0.00248826 0.00140209 0.00140209 0.00659463 0.00261936 0.00140209\n",
      "  0.00140209 0.00464853 0.00248826 0.00248826 0.00140209 0.00626457\n",
      "  0.00261936 0.00626457 0.00140209 0.00352998 0.01170335 0.00659463\n",
      "  0.00626457 0.00626457 0.00261936 0.00659463 0.00352998 0.00140209\n",
      "  0.00352998 0.00352998 0.00140209 0.00248826 0.00261936 0.00140209\n",
      "  0.00659463 0.00248826 0.00140209 0.00140209 0.00248826 0.00659463\n",
      "  0.00626457 0.00626457 0.00140209 0.00659463 0.00261936 0.00464853\n",
      "  0.00140209 0.00140209 0.00248826 0.00140209 0.00352998 0.00352998\n",
      "  0.00248826 0.00140209 0.00248826 0.00261936 0.00248826 0.00140209\n",
      "  0.00140209 0.00140209 0.00352998 0.00261936 0.00659463 0.00140209\n",
      "  0.00352998 0.00261936 0.00140209 0.00248826 0.01170335]]\n",
      "classEst:  [[-1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "   1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "   1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.\n",
      "   1.  1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1.\n",
      "  -1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.\n",
      "  -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1.  1.\n",
      "  -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.]]\n",
      "aggClassEst:  [[-0.66895971  0.72030514  0.82798452  0.25436505 -0.37059985  0.82798452\n",
      "   0.82798452  0.25436505 -0.66895971 -0.66895971  0.25436505  0.25436505\n",
      "   0.82798452  0.25436505  0.72030514 -0.66895971 -1.29392461  0.82798452\n",
      "   0.25436505 -0.37059985 -0.66895971  0.82798452  0.82798452  0.72030514\n",
      "   0.72030514  0.25436505  0.25436505  0.82798452  0.82798452  1.29392461\n",
      "  -1.29392461  0.25436505  0.25436505  0.25436505 -0.66895971 -1.29392461\n",
      "   0.25436505 -0.82798452  0.82798452  0.25436505 -1.29392461  0.25436505\n",
      "   0.25436505 -0.20301961 -0.82798452 -1.29392461 -1.29392461  0.82798452\n",
      "  -0.37059985 -1.29392461  0.82798452  0.25436505  0.82798452  0.82798452\n",
      "  -0.37059985  0.25436505  0.82798452 -0.66895971 -0.72030514 -1.29392461\n",
      "   0.82798452  0.25436505 -0.37059985  0.09534024  0.25436505  0.72030514\n",
      "   0.09534024  0.82798452  0.20301961  0.82798452  0.25436505  0.82798452\n",
      "   0.25436505 -0.72030514  0.82798452  0.25436505  1.29392461  1.29392461\n",
      "  -0.09534024 -0.37059985 -0.66895971  0.82798452  0.25436505 -0.72030514\n",
      "   0.25436505  0.82798452 -0.66895971  0.82798452  0.25436505  0.82798452\n",
      "   0.66895971 -0.37059985  0.82798452 -0.09534024  0.82798452  1.29392461\n",
      "   0.25436505 -0.37059985  0.82798452 -0.37059985  1.29392461 -0.66895971\n",
      "   0.82798452  0.25436505 -0.09534024  0.25436505  0.82798452  0.82798452\n",
      "  -0.82798452  0.25436505 -0.37059985  0.82798452  0.82798452  0.25436505\n",
      "   0.37059985  1.29392461  1.29392461  0.82798452 -0.20301961  0.72030514\n",
      "   0.82798452 -1.29392461  0.82798452 -0.66895971  0.82798452  0.82798452\n",
      "  -0.09534024 -0.09534024  0.25436505  0.82798452  0.82798452  0.25436505\n",
      "   0.82798452 -1.29392461 -0.66895971 -0.09534024  0.82798452  0.72030514\n",
      "   0.25436505  0.82798452  0.20301961 -1.29392461  0.82798452  0.09534024\n",
      "   0.72030514  1.29392461  0.82798452 -0.37059985  1.29392461  1.29392461\n",
      "  -0.72030514  0.09534024  0.82798452  1.29392461  0.82798452  0.82798452\n",
      "   0.72030514  0.82798452  0.82798452 -0.09534024  0.82798452  0.25436505\n",
      "  -0.66895971  0.25436505  0.09534024  0.82798452  0.82798452 -0.37059985\n",
      "  -0.66895971  0.82798452 -0.37059985  0.09534024 -0.37059985 -0.09534024\n",
      "   0.82798452  0.72030514 -0.66895971 -0.20301961 -0.66895971 -0.66895971\n",
      "   0.82798452 -0.72030514  0.72030514  0.72030514 -0.37059985  0.25436505\n",
      "   0.25436505  0.25436505  0.25436505  0.82798452 -0.66895971  1.29392461\n",
      "  -1.29392461  0.25436505  1.29392461 -0.37059985 -0.37059985  0.72030514\n",
      "   0.25436505  0.82798452 -0.66895971  0.82798452  0.82798452 -0.09534024\n",
      "  -0.66895971 -0.20301961  0.82798452  0.09534024  0.82798452  0.25436505\n",
      "   0.25436505  0.25436505  0.82798452  0.25436505  0.82798452  0.82798452\n",
      "  -0.37059985  0.82798452  0.82798452 -0.66895971  0.72030514 -0.37059985\n",
      "   0.25436505  0.82798452  0.82798452 -0.37059985  0.82798452 -1.29392461\n",
      "   0.25436505 -0.66895971 -1.29392461  0.82798452  0.25436505  0.82798452\n",
      "  -0.72030514  0.82798452  0.82798452  0.25436505 -0.20301961 -1.29392461\n",
      "   0.82798452 -0.09534024  0.25436505  0.25436505 -1.29392461 -0.66895971\n",
      "  -0.66895971 -0.66895971  0.82798452 -0.37059985  0.82798452  0.25436505\n",
      "  -0.66895971 -0.20301961 -0.66895971  0.72030514 -0.37059985  0.82798452\n",
      "  -0.37059985  0.37059985  0.82798452  0.25436505 -0.66895971  0.82798452\n",
      "   0.25436505  0.25436505  0.82798452  0.82798452  0.25436505  0.25436505\n",
      "  -0.66895971 -0.66895971 -1.29392461  0.25436505 -0.66895971  0.37059985\n",
      "   0.82798452  0.82798452  0.25436505  1.29392461 -0.37059985 -0.37059985\n",
      "   0.25436505  0.82798452  0.25436505 -0.66895971  0.25436505  0.82798452\n",
      "   0.82798452  0.82798452 -0.37059985 -0.66895971  0.25436505  1.29392461\n",
      "   0.09534024 -0.66895971 -1.29392461  0.25436505  0.82798452]]\n",
      "total error:  0.24749163879598662\n",
      "D: [[0.00213157 0.00855163 0.00181817 0.00536653 0.0028726  0.00181817\n",
      "  0.00181817 0.00536653 0.00213157 0.00812362 0.00322667 0.00322667\n",
      "  0.00181817 0.00536653 0.00202488 0.00213157 0.00114098 0.00181817\n",
      "  0.00322667 0.00602801 0.00812362 0.00181817 0.00181817 0.00202488\n",
      "  0.00202488 0.00322667 0.00322667 0.00181817 0.00952387 0.00114098\n",
      "  0.00114098 0.00536653 0.00322667 0.00322667 0.00812362 0.00114098\n",
      "  0.00536653 0.00952387 0.00181817 0.00536653 0.00114098 0.00322667\n",
      "  0.00322667 0.00339667 0.00181817 0.00114098 0.01517639 0.00181817\n",
      "  0.0028726  0.00114098 0.00181817 0.00536653 0.00181817 0.00181817\n",
      "  0.0028726  0.00536653 0.00181817 0.00213157 0.00202488 0.00114098\n",
      "  0.00181817 0.00322667 0.0028726  0.00457752 0.00322667 0.00202488\n",
      "  0.00378284 0.00181817 0.00339667 0.00181817 0.00536653 0.00181817\n",
      "  0.00536653 0.00202488 0.00952387 0.00536653 0.00114098 0.00114098\n",
      "  0.00378284 0.00602801 0.00213157 0.00181817 0.00322667 0.00202488\n",
      "  0.00322667 0.00181817 0.00213157 0.00952387 0.00322667 0.00181817\n",
      "  0.00213157 0.0028726  0.00181817 0.00378284 0.00181817 0.00114098\n",
      "  0.00322667 0.0028726  0.00181817 0.00602801 0.00114098 0.00213157\n",
      "  0.00181817 0.00322667 0.00378284 0.00322667 0.00181817 0.00181817\n",
      "  0.00181817 0.00536653 0.0028726  0.00181817 0.00181817 0.00536653\n",
      "  0.0028726  0.00114098 0.00114098 0.00952387 0.00509794 0.00202488\n",
      "  0.00181817 0.00114098 0.00181817 0.00213157 0.00952387 0.00181817\n",
      "  0.00457752 0.00457752 0.00536653 0.00181817 0.00952387 0.00536653\n",
      "  0.00181817 0.00114098 0.00213157 0.00378284 0.00181817 0.00202488\n",
      "  0.00322667 0.00181817 0.00509794 0.00114098 0.00181817 0.00457752\n",
      "  0.00202488 0.00114098 0.00952387 0.00602801 0.00114098 0.00114098\n",
      "  0.00202488 0.00378284 0.00181817 0.00114098 0.00181817 0.00181817\n",
      "  0.00855163 0.00181817 0.00181817 0.00457752 0.00952387 0.00322667\n",
      "  0.00213157 0.00322667 0.00457752 0.00181817 0.00181817 0.00602801\n",
      "  0.00213157 0.00181817 0.0028726  0.00378284 0.0028726  0.00378284\n",
      "  0.00181817 0.00202488 0.00213157 0.00509794 0.00213157 0.00213157\n",
      "  0.00181817 0.00202488 0.00202488 0.00202488 0.00602801 0.00322667\n",
      "  0.00322667 0.00322667 0.00322667 0.00181817 0.00812362 0.00114098\n",
      "  0.00114098 0.00322667 0.00114098 0.0028726  0.0028726  0.00202488\n",
      "  0.00322667 0.00181817 0.00213157 0.00952387 0.00181817 0.00457752\n",
      "  0.00213157 0.00509794 0.00952387 0.00457752 0.00181817 0.00536653\n",
      "  0.00536653 0.00322667 0.00181817 0.00322667 0.00181817 0.00181817\n",
      "  0.00602801 0.00952387 0.00181817 0.00213157 0.00202488 0.00602801\n",
      "  0.00322667 0.00181817 0.00181817 0.0028726  0.00181817 0.00114098\n",
      "  0.00322667 0.00812362 0.00114098 0.00181817 0.00322667 0.00181817\n",
      "  0.00202488 0.00181817 0.00181817 0.00536653 0.00339667 0.00114098\n",
      "  0.00181817 0.00378284 0.00322667 0.00322667 0.00114098 0.00812362\n",
      "  0.00213157 0.00812362 0.00181817 0.0028726  0.00952387 0.00536653\n",
      "  0.00812362 0.00509794 0.00213157 0.00855163 0.0028726  0.00181817\n",
      "  0.0028726  0.0028726  0.00181817 0.00322667 0.00213157 0.00181817\n",
      "  0.00536653 0.00322667 0.00181817 0.00181817 0.00322667 0.00536653\n",
      "  0.00812362 0.00812362 0.00114098 0.00536653 0.00213157 0.00602801\n",
      "  0.00181817 0.00181817 0.00322667 0.00114098 0.0028726  0.0028726\n",
      "  0.00322667 0.00181817 0.00322667 0.00213157 0.00322667 0.00181817\n",
      "  0.00181817 0.00181817 0.0028726  0.00213157 0.00536653 0.00114098\n",
      "  0.00457752 0.00213157 0.00114098 0.00322667 0.00952387]]\n",
      "classEst:  [[ 1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      "   1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.]]\n",
      "aggClassEst:  [[-0.47092125  0.91834361  1.02602298  0.45240351 -0.56863831  1.02602298\n",
      "   1.02602298  0.45240351 -0.47092125 -0.47092125  0.45240351  0.45240351\n",
      "   1.02602298  0.45240351  0.91834361 -0.47092125 -1.09588615  0.62994605\n",
      "   0.45240351 -0.17256139 -0.47092125  1.02602298  1.02602298  0.91834361\n",
      "   0.91834361  0.05632659  0.45240351  1.02602298  0.62994605  1.49196307\n",
      "  -1.09588615  0.45240351  0.45240351  0.45240351 -0.47092125 -1.09588615\n",
      "   0.05632659 -0.62994605  1.02602298  0.05632659 -1.09588615  0.05632659\n",
      "   0.45240351 -0.00498115 -0.62994605 -1.09588615 -1.09588615  1.02602298\n",
      "  -0.17256139 -1.09588615  1.02602298  0.45240351  1.02602298  0.62994605\n",
      "  -0.17256139  0.45240351  0.62994605 -0.47092125 -0.52226668 -1.09588615\n",
      "   1.02602298  0.45240351 -0.56863831  0.2933787   0.45240351  0.91834361\n",
      "   0.2933787   1.02602298  0.00498115  1.02602298  0.05632659  1.02602298\n",
      "   0.05632659 -0.52226668  0.62994605  0.45240351  1.49196307  1.49196307\n",
      "   0.10269822 -0.17256139 -0.47092125  1.02602298  0.45240351 -0.91834361\n",
      "   0.45240351  1.02602298 -0.47092125  0.62994605  0.45240351  1.02602298\n",
      "   0.47092125 -0.17256139  1.02602298  0.10269822  1.02602298  1.49196307\n",
      "   0.45240351 -0.17256139  1.02602298 -0.17256139  1.49196307 -0.47092125\n",
      "   0.62994605  0.45240351  0.10269822  0.45240351  0.62994605  1.02602298\n",
      "  -0.62994605  0.45240351 -0.17256139  1.02602298  1.02602298  0.05632659\n",
      "   0.56863831  1.49196307  1.49196307  1.02602298 -0.00498115  0.91834361\n",
      "   1.02602298 -1.09588615  1.02602298 -0.47092125  1.02602298  1.02602298\n",
      "   0.10269822  0.10269822  0.45240351  0.62994605  0.62994605  0.45240351\n",
      "   1.02602298 -1.09588615 -0.47092125  0.10269822  1.02602298  0.91834361\n",
      "   0.45240351  1.02602298  0.00498115 -1.09588615  1.02602298  0.2933787\n",
      "   0.91834361  1.49196307  1.02602298 -0.17256139  1.49196307  1.49196307\n",
      "  -0.52226668  0.2933787   1.02602298  1.49196307  1.02602298  1.02602298\n",
      "   0.91834361  1.02602298  1.02602298 -0.2933787   0.62994605  0.45240351\n",
      "  -0.47092125  0.45240351  0.2933787   1.02602298  1.02602298 -0.17256139\n",
      "  -0.86699817  1.02602298 -0.17256139  0.2933787  -0.17256139 -0.2933787\n",
      "   0.62994605  0.91834361 -0.86699817 -0.00498115 -0.47092125 -0.47092125\n",
      "   0.62994605 -0.91834361  0.91834361  0.91834361 -0.17256139  0.45240351\n",
      "   0.45240351  0.45240351  0.45240351  1.02602298 -0.47092125  1.49196307\n",
      "  -1.09588615  0.45240351  1.49196307 -0.17256139 -0.17256139  0.91834361\n",
      "   0.45240351  1.02602298 -0.47092125  0.62994605  1.02602298  0.10269822\n",
      "  -0.47092125 -0.00498115  0.62994605  0.2933787   1.02602298  0.45240351\n",
      "   0.45240351  0.45240351  1.02602298  0.45240351  1.02602298  0.62994605\n",
      "  -0.56863831  1.02602298  1.02602298 -0.47092125  0.91834361 -0.17256139\n",
      "   0.45240351  1.02602298  1.02602298 -0.17256139  0.62994605 -1.09588615\n",
      "   0.45240351 -0.47092125 -1.09588615  0.62994605  0.45240351  1.02602298\n",
      "  -0.52226668  1.02602298  1.02602298  0.45240351 -0.00498115 -1.09588615\n",
      "   1.02602298  0.10269822  0.45240351  0.45240351 -1.09588615 -0.47092125\n",
      "  -0.47092125 -0.47092125  1.02602298 -0.17256139  1.02602298  0.45240351\n",
      "  -0.47092125 -0.00498115 -0.47092125  0.91834361 -0.17256139  1.02602298\n",
      "  -0.17256139  0.56863831  0.62994605  0.05632659 -0.47092125  1.02602298\n",
      "   0.45240351  0.45240351  1.02602298  1.02602298  0.45240351  0.45240351\n",
      "  -0.47092125 -0.47092125 -1.09588615  0.45240351 -0.47092125  0.56863831\n",
      "   1.02602298  1.02602298  0.45240351  1.49196307 -0.17256139 -0.17256139\n",
      "   0.05632659  1.02602298  0.45240351 -0.86699817  0.05632659  0.62994605\n",
      "   1.02602298  1.02602298 -0.17256139 -0.47092125  0.45240351  1.49196307\n",
      "   0.2933787  -0.47092125 -1.09588615  0.45240351  0.62994605]]\n",
      "total error:  0.25418060200668896\n",
      "D: [[0.00264952 0.0106296  0.00152086 0.00667055 0.00240286 0.00152086\n",
      "  0.00152086 0.00667055 0.00264952 0.00679523 0.00269904 0.00269904\n",
      "  0.00152086 0.00667055 0.00169377 0.00264952 0.00141823 0.00225997\n",
      "  0.00269904 0.00504229 0.00679523 0.00152086 0.00152086 0.00169377\n",
      "  0.00169377 0.00401073 0.00269904 0.00152086 0.0079665  0.00095441\n",
      "  0.00141823 0.00667055 0.00269904 0.00269904 0.00679523 0.00141823\n",
      "  0.00448898 0.0079665  0.00152086 0.00448898 0.00141823 0.00401073\n",
      "  0.00269904 0.00422204 0.00225997 0.00141823 0.01269471 0.00152086\n",
      "  0.00357061 0.00141823 0.00152086 0.00667055 0.00152086 0.00225997\n",
      "  0.00357061 0.00667055 0.00225997 0.00264952 0.00251691 0.00141823\n",
      "  0.00152086 0.00269904 0.00240286 0.00568982 0.00269904 0.00169377\n",
      "  0.00316426 0.00152086 0.00422204 0.00152086 0.00448898 0.00152086\n",
      "  0.00448898 0.00251691 0.0079665  0.00667055 0.00095441 0.00095441\n",
      "  0.00470204 0.00504229 0.00264952 0.00152086 0.00269904 0.00169377\n",
      "  0.00269904 0.00152086 0.00264952 0.0079665  0.00269904 0.00152086\n",
      "  0.00264952 0.00357061 0.00152086 0.00470204 0.00152086 0.00095441\n",
      "  0.00269904 0.00357061 0.00152086 0.00504229 0.00095441 0.00264952\n",
      "  0.00225997 0.00269904 0.00470204 0.00269904 0.00225997 0.00152086\n",
      "  0.00225997 0.00667055 0.00357061 0.00152086 0.00152086 0.00448898\n",
      "  0.00240286 0.00095441 0.00095441 0.01183809 0.00426431 0.00169377\n",
      "  0.00152086 0.00141823 0.00152086 0.00264952 0.01183809 0.00152086\n",
      "  0.00382899 0.00382899 0.00667055 0.00225997 0.0079665  0.00667055\n",
      "  0.00152086 0.00141823 0.00264952 0.00470204 0.00152086 0.00169377\n",
      "  0.00269904 0.00152086 0.00426431 0.00141823 0.00152086 0.00568982\n",
      "  0.00169377 0.00095441 0.01183809 0.00504229 0.00095441 0.00095441\n",
      "  0.00251691 0.00316426 0.00152086 0.00095441 0.00152086 0.00152086\n",
      "  0.0106296  0.00152086 0.00152086 0.00568982 0.0079665  0.00269904\n",
      "  0.00264952 0.00269904 0.00568982 0.00152086 0.00152086 0.00504229\n",
      "  0.00178301 0.00152086 0.00357061 0.00316426 0.00357061 0.00316426\n",
      "  0.00225997 0.00169377 0.00178301 0.00426431 0.00264952 0.00264952\n",
      "  0.00225997 0.00169377 0.00169377 0.00169377 0.00504229 0.00269904\n",
      "  0.00269904 0.00269904 0.00269904 0.00152086 0.00679523 0.00095441\n",
      "  0.00141823 0.00269904 0.00095441 0.00357061 0.00357061 0.00169377\n",
      "  0.00269904 0.00152086 0.00264952 0.0079665  0.00152086 0.00382899\n",
      "  0.00264952 0.00426431 0.0079665  0.00568982 0.00152086 0.00667055\n",
      "  0.00667055 0.00269904 0.00152086 0.00269904 0.00152086 0.00225997\n",
      "  0.00749276 0.01183809 0.00152086 0.00264952 0.00169377 0.00504229\n",
      "  0.00269904 0.00152086 0.00152086 0.00357061 0.00225997 0.00141823\n",
      "  0.00269904 0.00679523 0.00141823 0.00225997 0.00269904 0.00152086\n",
      "  0.00251691 0.00152086 0.00152086 0.00667055 0.00422204 0.00141823\n",
      "  0.00152086 0.00470204 0.00269904 0.00269904 0.00141823 0.00679523\n",
      "  0.00264952 0.00679523 0.00152086 0.00357061 0.01183809 0.00667055\n",
      "  0.00679523 0.00426431 0.00264952 0.0106296  0.00357061 0.00152086\n",
      "  0.00357061 0.00240286 0.00225997 0.00401073 0.00264952 0.00152086\n",
      "  0.00667055 0.00269904 0.00152086 0.00152086 0.00269904 0.00667055\n",
      "  0.00679523 0.00679523 0.00141823 0.00667055 0.00264952 0.00749276\n",
      "  0.00152086 0.00152086 0.00269904 0.00095441 0.00357061 0.00357061\n",
      "  0.00401073 0.00152086 0.00269904 0.00178301 0.00401073 0.00225997\n",
      "  0.00152086 0.00152086 0.00357061 0.00264952 0.00667055 0.00095441\n",
      "  0.00568982 0.00264952 0.00141823 0.00269904 0.0079665 ]]\n",
      "classEst:  [[-1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "   1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1. -1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1.\n",
      "   1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  -1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.\n",
      "  -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1. -1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.\n",
      "  -1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      "   1.  1. -1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1. -1.\n",
      "   1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
      "  -1. -1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.\n",
      "  -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.65940012  1.10682248  1.21450185  0.26392464 -0.38015944  1.21450185\n",
      "   1.21450185  0.26392464 -0.65940012 -0.28244237  0.26392464  0.64088239\n",
      "   1.21450185  0.64088239  0.72986473 -0.65940012 -1.28436502  0.81842493\n",
      "   0.64088239 -0.36104026 -0.28244237  1.21450185  1.21450185  1.10682248\n",
      "   0.72986473  0.24480546  0.26392464  0.8375441   0.81842493  1.68044194\n",
      "  -1.28436502  0.26392464  0.64088239  0.64088239 -0.65940012 -1.28436502\n",
      "   0.24480546 -0.44146718  1.21450185  0.24480546 -1.28436502  0.24480546\n",
      "   0.26392464 -0.19346003 -0.81842493 -1.28436502 -1.28436502  1.21450185\n",
      "  -0.36104026 -1.28436502  1.21450185  0.64088239  1.21450185  0.81842493\n",
      "  -0.36104026  0.26392464  0.81842493 -0.28244237 -0.71074556 -1.28436502\n",
      "   1.21450185  0.64088239 -0.38015944  0.10489983  0.26392464  0.72986473\n",
      "   0.48185758  1.21450185  0.19346003  1.21450185 -0.13215228  1.21450185\n",
      "   0.24480546 -0.33378781  0.81842493  0.26392464  1.3034842   1.68044194\n",
      "  -0.08578066  0.01591749 -0.65940012  1.21450185  0.26392464 -1.10682248\n",
      "   0.64088239  1.21450185 -0.65940012  0.81842493  0.64088239  1.21450185\n",
      "   0.65940012 -0.36104026  0.8375441  -0.08578066  0.8375441   1.68044194\n",
      "   0.64088239 -0.36104026  1.21450185 -0.36104026  1.3034842  -0.65940012\n",
      "   0.44146718  0.64088239  0.29117709  0.64088239  0.81842493  1.21450185\n",
      "  -0.81842493  0.26392464 -0.36104026  0.8375441   1.21450185  0.24480546\n",
      "   0.38015944  1.3034842   1.68044194  0.8375441   0.18349772  1.10682248\n",
      "   1.21450185 -0.90740727  1.21450185 -0.28244237  0.8375441   0.8375441\n",
      "  -0.08578066 -0.08578066  0.64088239  0.81842493  0.81842493  0.26392464\n",
      "   1.21450185 -1.28436502 -0.65940012 -0.08578066  1.21450185  1.10682248\n",
      "   0.64088239  0.8375441   0.19346003 -1.28436502  1.21450185  0.10489983\n",
      "   0.72986473  1.68044194  1.21450185  0.01591749  1.68044194  1.3034842\n",
      "  -0.71074556  0.10489983  0.8375441   1.68044194  0.8375441   1.21450185\n",
      "   1.10682248  0.8375441   1.21450185 -0.10489983  0.81842493  0.26392464\n",
      "  -0.28244237  0.64088239  0.48185758  1.21450185  1.21450185  0.01591749\n",
      "  -0.6785193   1.21450185 -0.36104026  0.10489983 -0.36104026 -0.48185758\n",
      "   0.81842493  1.10682248 -0.6785193  -0.19346003 -0.65940012 -0.65940012\n",
      "   0.81842493 -1.10682248  1.10682248  1.10682248 -0.36104026  0.64088239\n",
      "   0.26392464  0.64088239  0.26392464  1.21450185 -0.28244237  1.68044194\n",
      "  -1.28436502  0.64088239  1.3034842   0.01591749 -0.36104026  1.10682248\n",
      "   0.26392464  1.21450185 -0.28244237  0.81842493  1.21450185 -0.08578066\n",
      "  -0.65940012  0.18349772  0.81842493  0.10489983  0.8375441   0.64088239\n",
      "   0.26392464  0.64088239  0.8375441   0.64088239  1.21450185  0.81842493\n",
      "  -0.38015944  1.21450185  0.8375441  -0.65940012  0.72986473 -0.36104026\n",
      "   0.64088239  1.21450185  1.21450185 -0.36104026  0.81842493 -1.28436502\n",
      "   0.64088239 -0.65940012 -1.28436502  0.81842493  0.26392464  0.8375441\n",
      "  -0.33378781  1.21450185  1.21450185  0.26392464  0.18349772 -1.28436502\n",
      "   1.21450185  0.29117709  0.64088239  0.64088239 -1.28436502 -0.65940012\n",
      "  -0.65940012 -0.28244237  1.21450185 -0.36104026  0.8375441   0.26392464\n",
      "  -0.65940012 -0.19346003 -0.28244237  1.10682248 -0.36104026  1.21450185\n",
      "  -0.36104026  0.75711718  0.81842493  0.24480546 -0.28244237  0.8375441\n",
      "   0.26392464  0.64088239  0.8375441   1.21450185  0.64088239  0.26392464\n",
      "  -0.65940012 -0.65940012 -1.28436502  0.64088239 -0.28244237  0.38015944\n",
      "   1.21450185  1.21450185  0.26392464  1.68044194 -0.36104026 -0.36104026\n",
      "   0.24480546  1.21450185  0.64088239 -1.05547704  0.24480546  0.81842493\n",
      "   1.21450185  1.21450185 -0.36104026 -0.28244237  0.26392464  1.68044194\n",
      "   0.10489983 -0.65940012 -1.28436502  0.26392464  0.81842493]]\n",
      "total error:  0.2408026755852843\n",
      "D: [[0.00223347 0.01306294 0.00128204 0.00562309 0.00295293 0.00128204\n",
      "  0.00128204 0.00562309 0.00223347 0.00572819 0.00331691 0.00227522\n",
      "  0.00128204 0.00819758 0.00208151 0.00223347 0.00119553 0.0019051\n",
      "  0.00227522 0.00619658 0.00572819 0.00128204 0.00128204 0.0014278\n",
      "  0.00208151 0.00338093 0.00331691 0.00186902 0.0097902  0.00080454\n",
      "  0.00119553 0.00562309 0.00227522 0.00227522 0.0083508  0.00119553\n",
      "  0.0055166  0.00671554 0.00128204 0.0055166  0.00119553 0.00338093\n",
      "  0.00331691 0.00355906 0.0019051  0.00119553 0.0156008  0.00128204\n",
      "  0.00300993 0.00119553 0.00128204 0.00819758 0.00128204 0.0019051\n",
      "  0.00300993 0.00562309 0.0019051  0.00325605 0.00212169 0.00119553\n",
      "  0.00128204 0.00227522 0.00295293 0.00479636 0.00331691 0.00208151\n",
      "  0.00266739 0.00128204 0.00355906 0.00128204 0.00378409 0.00128204\n",
      "  0.0055166  0.00309309 0.0097902  0.00562309 0.00117289 0.00080454\n",
      "  0.00396369 0.00425051 0.00223347 0.00128204 0.00331691 0.0014278\n",
      "  0.00227522 0.00128204 0.00223347 0.0097902  0.00227522 0.00128204\n",
      "  0.00223347 0.00300993 0.00186902 0.00396369 0.00186902 0.00080454\n",
      "  0.00227522 0.00300993 0.00128204 0.00619658 0.00117289 0.00223347\n",
      "  0.00277733 0.00227522 0.00577844 0.00227522 0.0019051  0.00128204\n",
      "  0.0019051  0.00562309 0.00300993 0.00186902 0.00128204 0.0055166\n",
      "  0.00295293 0.00117289 0.00080454 0.00997918 0.0035947  0.0014278\n",
      "  0.00128204 0.0017429  0.00128204 0.00325605 0.00997918 0.00186902\n",
      "  0.00470553 0.00470553 0.00819758 0.0019051  0.0097902  0.00562309\n",
      "  0.00128204 0.00119553 0.00223347 0.00396369 0.00128204 0.0014278\n",
      "  0.00227522 0.00186902 0.0052405  0.00119553 0.00128204 0.00479636\n",
      "  0.00208151 0.00080454 0.01454808 0.00425051 0.00080454 0.00117289\n",
      "  0.00212169 0.00388863 0.00186902 0.00080454 0.00186902 0.00128204\n",
      "  0.01306294 0.00186902 0.00128204 0.00479636 0.0097902  0.00331691\n",
      "  0.00325605 0.00227522 0.00699234 0.00128204 0.00128204 0.00425051\n",
      "  0.00219117 0.00128204 0.00300993 0.00388863 0.00300993 0.00266739\n",
      "  0.0019051  0.0014278  0.00219117 0.0052405  0.00223347 0.00223347\n",
      "  0.0019051  0.0014278  0.0014278  0.0014278  0.00619658 0.00227522\n",
      "  0.00331691 0.00227522 0.00331691 0.00128204 0.00572819 0.00080454\n",
      "  0.00119553 0.00227522 0.00117289 0.004388   0.00300993 0.0014278\n",
      "  0.00331691 0.00128204 0.00325605 0.0097902  0.00128204 0.00470553\n",
      "  0.00223347 0.0035947  0.0097902  0.00479636 0.00186902 0.00819758\n",
      "  0.00562309 0.00227522 0.00186902 0.00227522 0.00128204 0.0019051\n",
      "  0.00631619 0.01454808 0.00186902 0.00223347 0.00208151 0.00619658\n",
      "  0.00227522 0.00128204 0.00128204 0.00300993 0.0019051  0.00119553\n",
      "  0.00227522 0.0083508  0.00119553 0.0019051  0.00331691 0.00186902\n",
      "  0.00309309 0.00128204 0.00128204 0.00562309 0.00518855 0.00119553\n",
      "  0.00128204 0.00577844 0.00227522 0.00227522 0.00119553 0.0083508\n",
      "  0.00223347 0.00572819 0.00128204 0.00300993 0.00997918 0.00562309\n",
      "  0.0083508  0.0052405  0.00325605 0.01306294 0.00300993 0.00128204\n",
      "  0.00300993 0.00202555 0.0019051  0.00338093 0.00325605 0.00186902\n",
      "  0.00562309 0.00227522 0.00186902 0.00128204 0.00227522 0.00562309\n",
      "  0.0083508  0.0083508  0.00119553 0.00819758 0.00325605 0.00631619\n",
      "  0.00128204 0.00128204 0.00331691 0.00080454 0.00300993 0.00300993\n",
      "  0.00338093 0.00128204 0.00227522 0.00150303 0.00338093 0.0019051\n",
      "  0.00128204 0.00128204 0.00300993 0.00325605 0.00562309 0.00080454\n",
      "  0.00479636 0.00223347 0.00119553 0.00331691 0.0097902 ]]\n",
      "classEst:  [[-1. -1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      "   1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1.\n",
      "  -1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.\n",
      "  -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      "  -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      "  -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1.\n",
      "   1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  -1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      "  -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      "  -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  -1. -1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1.\n",
      "   1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.  1.\n",
      "   1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  -1. -1.  1. -1.  1. -1. -1.  1.  1.  1. -1.]]\n",
      "aggClassEst:  [[-0.81167381  0.95454879  1.06222816  0.11165095 -0.53243313  1.36677554\n",
      "   1.06222816  0.41619833 -0.50712643 -0.13016868  0.41619833  0.4886087\n",
      "   1.36677554  0.79315608  0.88213842 -0.50712643 -1.13209133  0.66615124\n",
      "   0.4886087  -0.51331395 -0.13016868  1.36677554  1.06222816  0.95454879\n",
      "   0.88213842  0.09253177  0.41619833  0.68527041  0.66615124  1.52816825\n",
      "  -1.13209133  0.11165095  0.79315608  0.4886087  -0.50712643 -1.43663871\n",
      "   0.09253177 -0.59374087  1.06222816  0.09253177 -1.43663871  0.09253177\n",
      "   0.11165095 -0.34573372 -0.66615124 -1.43663871 -1.13209133  1.06222816\n",
      "  -0.20876657 -1.43663871  1.06222816  0.4886087   1.06222816  0.66615124\n",
      "  -0.20876657  0.41619833  0.66615124 -0.13016868 -0.55847187 -1.43663871\n",
      "   1.06222816  0.79315608 -0.22788575 -0.04737386  0.11165095  0.57759104\n",
      "   0.32958389  1.36677554  0.34573372  1.06222816  0.02012141  1.36677554\n",
      "   0.09253177 -0.18151412  0.66615124  0.11165095  1.15121051  1.52816825\n",
      "   0.06649303  0.16819118 -0.81167381  1.36677554  0.11165095 -1.25909617\n",
      "   0.4886087   1.36677554 -0.50712643  0.66615124  0.79315608  1.06222816\n",
      "   0.50712643 -0.20876657  0.98981779 -0.23805435  0.68527041  1.83271563\n",
      "   0.79315608 -0.51331395  1.06222816 -0.51331395  1.15121051 -0.81167381\n",
      "   0.28919349  0.4886087   0.44345078  0.79315608  0.66615124  1.06222816\n",
      "  -0.97069862  0.11165095 -0.20876657  0.68527041  1.06222816  0.09253177\n",
      "   0.53243313  1.15121051  1.83271563  0.68527041  0.03122403  0.95454879\n",
      "   1.06222816 -1.05968096  1.06222816 -0.13016868  0.98981779  0.98981779\n",
      "  -0.23805435  0.06649303  0.4886087   0.97069862  0.66615124  0.11165095\n",
      "   1.06222816 -1.13209133 -0.50712643  0.06649303  1.36677554  0.95454879\n",
      "   0.4886087   0.68527041  0.04118634 -1.43663871  1.36677554  0.25717352\n",
      "   0.88213842  1.52816825  1.06222816  0.16819118  1.52816825  1.15121051\n",
      "  -0.86301925  0.25717352  0.98981779  1.83271563  0.98981779  1.06222816\n",
      "   1.25909617  0.68527041  1.36677554 -0.25717352  0.66615124  0.41619833\n",
      "  -0.43471606  0.79315608  0.32958389  1.06222816  1.36677554  0.16819118\n",
      "  -0.52624561  1.06222816 -0.51331395 -0.04737386 -0.51331395 -0.63413127\n",
      "   0.66615124  1.25909617 -0.83079299 -0.34573372 -0.50712643 -0.50712643\n",
      "   0.66615124 -1.25909617  1.25909617  1.25909617 -0.51331395  0.4886087\n",
      "   0.41619833  0.4886087   0.41619833  1.06222816 -0.43471606  1.52816825\n",
      "  -1.43663871  0.4886087   1.15121051  0.16819118 -0.51331395  0.95454879\n",
      "   0.11165095  1.36677554 -0.13016868  0.66615124  1.36677554  0.06649303\n",
      "  -0.50712643  0.33577141  0.66615124  0.25717352  0.98981779  0.4886087\n",
      "   0.11165095  0.79315608  0.98981779  0.79315608  1.36677554  0.66615124\n",
      "  -0.53243313  1.06222816  0.98981779 -0.81167381  0.57759104 -0.20876657\n",
      "   0.79315608  1.06222816  1.36677554 -0.51331395  0.66615124 -1.13209133\n",
      "   0.79315608 -0.81167381 -1.43663871  0.66615124  0.41619833  0.68527041\n",
      "  -0.18151412  1.06222816  1.06222816  0.41619833  0.03122403 -1.43663871\n",
      "   1.06222816  0.44345078  0.4886087   0.4886087  -1.43663871 -0.50712643\n",
      "  -0.50712643 -0.13016868  1.36677554 -0.51331395  0.68527041  0.11165095\n",
      "  -0.81167381 -0.34573372 -0.43471606  0.95454879 -0.51331395  1.36677554\n",
      "  -0.20876657  0.60484349  0.97069862  0.09253177 -0.13016868  0.98981779\n",
      "   0.41619833  0.4886087   0.68527041  1.06222816  0.4886087   0.41619833\n",
      "  -0.50712643 -0.50712643 -1.43663871  0.4886087  -0.43471606  0.53243313\n",
      "   1.36677554  1.06222816  0.41619833  1.52816825 -0.51331395 -0.20876657\n",
      "   0.09253177  1.36677554  0.79315608 -1.20775073  0.09253177  0.66615124\n",
      "   1.06222816  1.06222816 -0.20876657 -0.43471606  0.41619833  1.52816825\n",
      "  -0.04737386 -0.50712643 -1.13209133  0.41619833  0.66615124]]\n",
      "total error:  0.2408026755852843\n",
      "D: [[0.00194028 0.01134815 0.00151025 0.00488494 0.00256529 0.00111375\n",
      "  0.00151025 0.00662403 0.00263104 0.00497624 0.00288149 0.00268022\n",
      "  0.00111375 0.0096568  0.00180826 0.00263104 0.00140834 0.00224421\n",
      "  0.00268022 0.0072996  0.00497624 0.00111375 0.00151025 0.00168196\n",
      "  0.00180826 0.00398276 0.00288149 0.00220171 0.00850503 0.00094775\n",
      "  0.00140834 0.00488494 0.00197654 0.00268022 0.00725458 0.00103859\n",
      "  0.00479243 0.00791094 0.00151025 0.00479243 0.00103859 0.00398276\n",
      "  0.00390733 0.00309186 0.00224421 0.00103859 0.01355286 0.00151025\n",
      "  0.00354571 0.00103859 0.00151025 0.00712147 0.00151025 0.00224421\n",
      "  0.00354571 0.00662403 0.00224421 0.00383564 0.00249936 0.00103859\n",
      "  0.00151025 0.00197654 0.00347857 0.00416673 0.00390733 0.00245203\n",
      "  0.0031422  0.00111375 0.00309186 0.00151025 0.00445768 0.00111375\n",
      "  0.00479243 0.00364367 0.00850503 0.00488494 0.00138167 0.00094775\n",
      "  0.00466925 0.00369254 0.00194028 0.00111375 0.00390733 0.00124037\n",
      "  0.00268022 0.00111375 0.00263104 0.00850503 0.00197654 0.00151025\n",
      "  0.00263104 0.00354571 0.00162367 0.00344337 0.00220171 0.00069893\n",
      "  0.00197654 0.00261481 0.00151025 0.0072996  0.00138167 0.00194028\n",
      "  0.00327171 0.00268022 0.00680704 0.00197654 0.00224421 0.00151025\n",
      "  0.00165501 0.00488494 0.00354571 0.00220171 0.00151025 0.00479243\n",
      "  0.00256529 0.00138167 0.00069893 0.0086692  0.00423457 0.00168196\n",
      "  0.00151025 0.00151411 0.00151025 0.00383564 0.01175553 0.00162367\n",
      "  0.00554314 0.00408783 0.00712147 0.00165501 0.00850503 0.00488494\n",
      "  0.00151025 0.00140834 0.00263104 0.00466925 0.00111375 0.00168196\n",
      "  0.00268022 0.00220171 0.00455257 0.00103859 0.00111375 0.00565014\n",
      "  0.00180826 0.00094775 0.01263833 0.00369254 0.00094775 0.00138167\n",
      "  0.00184317 0.00337816 0.00162367 0.00069893 0.00162367 0.00151025\n",
      "  0.01538822 0.00220171 0.00111375 0.00565014 0.00850503 0.00288149\n",
      "  0.00282862 0.00197654 0.00607444 0.00151025 0.00111375 0.00369254\n",
      "  0.00258122 0.00151025 0.00261481 0.00458083 0.00261481 0.00231724\n",
      "  0.00224421 0.00124037 0.00190354 0.00617334 0.00263104 0.00263104\n",
      "  0.00224421 0.00124037 0.00124037 0.00124037 0.0072996  0.00268022\n",
      "  0.00288149 0.00268022 0.00288149 0.00151025 0.00674784 0.00094775\n",
      "  0.00103859 0.00268022 0.00138167 0.00516909 0.00261481 0.00168196\n",
      "  0.00390733 0.00111375 0.00383564 0.00850503 0.00111375 0.00408783\n",
      "  0.00263104 0.00312281 0.00850503 0.00565014 0.00162367 0.00712147\n",
      "  0.00488494 0.00197654 0.00162367 0.00197654 0.00111375 0.00224421\n",
      "  0.00744051 0.01263833 0.00162367 0.00194028 0.00245203 0.00538314\n",
      "  0.00197654 0.00151025 0.00111375 0.00261481 0.00224421 0.00140834\n",
      "  0.00197654 0.00983729 0.00103859 0.00224421 0.00288149 0.00220171\n",
      "  0.00364367 0.00151025 0.00151025 0.00662403 0.00450744 0.00103859\n",
      "  0.00151025 0.00680704 0.00268022 0.00268022 0.00103859 0.00725458\n",
      "  0.00263104 0.00497624 0.00111375 0.00261481 0.0086692  0.00488494\n",
      "  0.00983729 0.00617334 0.00282862 0.01134815 0.00261481 0.00111375\n",
      "  0.00354571 0.00238611 0.00165501 0.00398276 0.00383564 0.00162367\n",
      "  0.00662403 0.00268022 0.00220171 0.00151025 0.00268022 0.00662403\n",
      "  0.00725458 0.00725458 0.00103859 0.00712147 0.00282862 0.00744051\n",
      "  0.00111375 0.00151025 0.00288149 0.00094775 0.00261481 0.00354571\n",
      "  0.00398276 0.00111375 0.00197654 0.00130572 0.00398276 0.00224421\n",
      "  0.00151025 0.00151025 0.00354571 0.00282862 0.00662403 0.00094775\n",
      "  0.00416673 0.00263104 0.00140834 0.00288149 0.00850503]]\n",
      "classEst:  [[ 1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  -1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.\n",
      "   1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1.\n",
      "   1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  -1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1.\n",
      "  -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.\n",
      "   1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1.  1.\n",
      "   1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      "  -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.\n",
      "  -1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.6565651   0.79944008  0.90711945 -0.04345776 -0.68754184  1.21166683\n",
      "   1.21733687  0.57130704 -0.66223514  0.02494003  0.26108962  0.33349999\n",
      "   1.52188425  0.63804737  1.03724713 -0.66223514 -1.28720004  0.82125995\n",
      "   0.33349999 -0.66842266 -0.28527739  1.52188425  0.90711945  1.1096575\n",
      "   0.72702971  0.24764048  0.26108962  0.84037912  0.82125995  1.68327696\n",
      "  -1.28720004 -0.04345776  0.94826478  0.6437174  -0.66223514 -1.59174742\n",
      "  -0.06257693 -0.43863216  1.21733687  0.24764048 -1.59174742  0.24764048\n",
      "  -0.04345776 -0.50084242 -0.82125995 -1.59174742 -1.28720004  1.21733687\n",
      "  -0.36387528 -1.59174742  0.90711945  0.33349999  0.90711945  0.82125995\n",
      "  -0.05365786  0.26108962  0.82125995 -0.28527739 -0.71358057 -1.59174742\n",
      "   0.90711945  0.94826478 -0.38299446 -0.20248257 -0.04345776  0.42248233\n",
      "   0.17447518  1.52188425  0.19062501  1.21733687  0.17523011  1.52188425\n",
      "   0.24764048 -0.33662283  0.82125995 -0.04345776  1.30631921  1.68327696\n",
      "  -0.08861567  0.01308247 -0.96678252  1.52188425 -0.04345776 -1.41420488\n",
      "   0.33349999  1.21166683 -0.66223514  0.82125995  0.63804737  1.21733687\n",
      "   0.66223514 -0.05365786  1.1449265  -0.08294564  0.53016171  1.67760692\n",
      "   0.63804737 -0.66842266  0.90711945 -0.66842266  1.30631921 -0.96678252\n",
      "   0.4443022   0.6437174   0.28834207  0.63804737  0.51104253  0.90711945\n",
      "  -1.12580733 -0.04345776 -0.36387528  0.84037912  1.21733687  0.24764048\n",
      "   0.37732442  1.30631921  1.98782434  0.53016171  0.18633274  0.79944008\n",
      "   0.90711945 -1.21478967  1.21733687 -0.28527739  0.83470909  1.1449265\n",
      "  -0.39316305  0.22160174  0.33349999  1.12580733  0.82125995  0.26675966\n",
      "   1.21733687 -1.28720004 -0.66223514 -0.08861567  1.52188425  0.79944008\n",
      "   0.6437174   0.84037912 -0.11392237 -1.59174742  1.52188425  0.10206481\n",
      "   0.72702971  1.37305954  0.90711945  0.01308247  1.68327696  1.30631921\n",
      "  -1.01812795  0.10206481  1.1449265   1.67760692  0.83470909  1.21733687\n",
      "   1.10398746  0.84037912  1.21166683 -0.10206481  0.82125995  0.57130704\n",
      "  -0.58982477  0.63804737  0.17447518  1.21733687  1.52188425  0.01308247\n",
      "  -0.3711369   0.90711945 -0.66842266 -0.20248257 -0.66842266 -0.78923998\n",
      "   0.82125995  1.10398746 -0.98590169 -0.50084242 -0.66223514 -0.66223514\n",
      "   0.82125995 -1.41420488  1.10398746  1.10398746 -0.66842266  0.6437174\n",
      "   0.57130704  0.6437174   0.57130704  0.90711945 -0.27960735  1.68327696\n",
      "  -1.59174742  0.33349999  1.30631921  0.01308247 -0.35820524  1.1096575\n",
      "  -0.04345776  1.21166683 -0.28527739  0.82125995  1.52188425 -0.08861567\n",
      "  -0.66223514  0.1806627   0.82125995  0.10206481  1.1449265   0.33349999\n",
      "  -0.04345776  0.63804737  0.83470909  0.94826478  1.52188425  0.82125995\n",
      "  -0.37732442  0.90711945  0.83470909 -0.6565651   0.73269975 -0.36387528\n",
      "   0.63804737  1.21733687  1.52188425 -0.66842266  0.82125995 -1.28720004\n",
      "   0.94826478 -0.6565651  -1.59174742  0.82125995  0.26108962  0.53016171\n",
      "  -0.33662283  1.21733687  0.90711945  0.26108962  0.18633274 -1.59174742\n",
      "   0.90711945  0.28834207  0.6437174   0.33349999 -1.59174742 -0.66223514\n",
      "  -0.66223514 -0.28527739  1.52188425 -0.66842266  0.53016171 -0.04345776\n",
      "  -0.96678252 -0.50084242 -0.27960735  1.1096575  -0.66842266  1.52188425\n",
      "  -0.05365786  0.7599522   1.12580733  0.24764048 -0.28527739  0.83470909\n",
      "   0.57130704  0.33349999  0.53016171  1.21733687  0.6437174   0.57130704\n",
      "  -0.66223514 -0.66223514 -1.59174742  0.6437174  -0.58982477  0.68754184\n",
      "   1.21166683  1.21733687  0.57130704  1.68327696 -0.66842266 -0.36387528\n",
      "   0.24764048  1.52188425  0.94826478 -1.05264202  0.24764048  0.82125995\n",
      "   1.21733687  0.90711945 -0.36387528 -0.58982477  0.26108962  1.68327696\n",
      "  -0.20248257 -0.66223514 -1.28720004  0.26108962  0.82125995]]\n",
      "total error:  0.22073578595317725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[0.00229314 0.0098348  0.00178491 0.0042335  0.0022232  0.00131629\n",
      "  0.00130885 0.00782869 0.00228018 0.00431263 0.00340552 0.00316764\n",
      "  0.00096522 0.008369   0.00156712 0.00228018 0.00122053 0.00194493\n",
      "  0.00316764 0.00862712 0.00588122 0.00096522 0.00178491 0.00145766\n",
      "  0.00213712 0.00345163 0.00340552 0.0019081  0.01005176 0.00082136\n",
      "  0.00122053 0.0042335  0.00171296 0.00232279 0.0085739  0.00090009\n",
      "  0.00415333 0.00685597 0.00130885 0.00566399 0.00090009 0.00345163\n",
      "  0.00461792 0.00267954 0.00194493 0.00090009 0.01601759 0.00130885\n",
      "  0.00307287 0.00090009 0.00178491 0.00617178 0.00178491 0.00194493\n",
      "  0.00419054 0.00574068 0.00194493 0.00332414 0.00216605 0.00090009\n",
      "  0.00178491 0.00171296 0.00301468 0.00361107 0.00461792 0.00289795\n",
      "  0.00371364 0.00096522 0.00365415 0.00130885 0.00526835 0.00096522\n",
      "  0.00566399 0.00315777 0.01005176 0.0042335  0.00119742 0.00082136\n",
      "  0.00404658 0.00436407 0.00168153 0.00096522 0.00461792 0.00107496\n",
      "  0.00316764 0.00131629 0.00228018 0.01005176 0.002336   0.00130885\n",
      "  0.00228018 0.00419054 0.00140714 0.00406959 0.00260212 0.00082603\n",
      "  0.002336   0.00226611 0.00178491 0.00862712 0.00119742 0.00168153\n",
      "  0.00283541 0.00232279 0.00589928 0.002336   0.00265235 0.00178491\n",
      "  0.0014343  0.0042335  0.00307287 0.0019081  0.00130885 0.00566399\n",
      "  0.00303182 0.00119742 0.00060572 0.00751311 0.00366986 0.00198784\n",
      "  0.00178491 0.00131219 0.00130885 0.00332414 0.01018786 0.00140714\n",
      "  0.00655122 0.00354269 0.00617178 0.0014343  0.01005176 0.00577332\n",
      "  0.00130885 0.00122053 0.00228018 0.00404658 0.00096522 0.00198784\n",
      "  0.00232279 0.0019081  0.00394546 0.00090009 0.00096522 0.00489666\n",
      "  0.00213712 0.00112011 0.01095293 0.00436407 0.00082136 0.00119742\n",
      "  0.00159737 0.00399252 0.00140714 0.00082603 0.00191895 0.00130885\n",
      "  0.0133361  0.0019081  0.00131629 0.00489666 0.01005176 0.00249723\n",
      "  0.00245141 0.002336   0.00526438 0.00130885 0.00096522 0.00436407\n",
      "  0.00305064 0.00178491 0.00226611 0.0054139  0.00226611 0.00200822\n",
      "  0.00194493 0.00146594 0.00164969 0.00729603 0.00228018 0.00228018\n",
      "  0.00194493 0.00107496 0.00146594 0.00146594 0.00862712 0.00232279\n",
      "  0.00249723 0.00232279 0.00249723 0.00178491 0.00584797 0.00082136\n",
      "  0.00090009 0.00316764 0.00119742 0.00447976 0.00309034 0.00145766\n",
      "  0.00461792 0.00131629 0.00332414 0.01005176 0.00096522 0.00483124\n",
      "  0.00228018 0.00369073 0.01005176 0.00489666 0.00140714 0.00617178\n",
      "  0.0042335  0.002336   0.00191895 0.00171296 0.00096522 0.00194493\n",
      "  0.00644827 0.01095293 0.00191895 0.00229314 0.00212503 0.00636213\n",
      "  0.002336   0.00130885 0.00096522 0.00226611 0.00194493 0.00122053\n",
      "  0.00171296 0.00852542 0.00090009 0.00194493 0.00340552 0.00260212\n",
      "  0.00315777 0.00130885 0.00178491 0.00574068 0.00532717 0.00090009\n",
      "  0.00178491 0.00589928 0.00232279 0.00316764 0.00090009 0.0085739\n",
      "  0.00228018 0.00588122 0.00096522 0.00226611 0.00751311 0.0042335\n",
      "  0.0116263  0.00729603 0.00334304 0.01341194 0.00226611 0.00096522\n",
      "  0.00419054 0.0020679  0.0014343  0.00345163 0.00332414 0.00191895\n",
      "  0.00782869 0.00316764 0.00260212 0.00130885 0.00232279 0.00782869\n",
      "  0.0085739  0.0085739  0.00090009 0.00841659 0.00245141 0.00879365\n",
      "  0.00131629 0.00130885 0.00249723 0.00082136 0.00226611 0.00307287\n",
      "  0.00345163 0.00096522 0.00171296 0.00154318 0.00345163 0.00194493\n",
      "  0.00130885 0.00178491 0.00307287 0.00245141 0.00574068 0.00082136\n",
      "  0.00361107 0.00228018 0.00122053 0.00340552 0.01005176]]\n",
      "classEst:  [[ 1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "   1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      "   1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "   1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "  -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.]]\n",
      "aggClassEst:  [[-0.52120313  0.66407811  1.04248143  0.09190421 -0.82290381  1.34702881\n",
      "   1.35269884  0.70666901 -0.52687316  0.160302    0.39645159  0.46886196\n",
      "   1.65724622  0.77340934  1.1726091  -0.52687316 -1.15183806  0.68589797\n",
      "   0.46886196 -0.53306069 -0.14991542  1.65724622  1.04248143  0.97429552\n",
      "   0.86239169  0.11227851  0.39645159  0.9757411   0.68589797  1.81863893\n",
      "  -1.15183806  0.09190421  1.08362676  0.77907938 -0.52687316 -1.45638544\n",
      "  -0.19793891 -0.57399414  1.35269884  0.11227851 -1.45638544  0.11227851\n",
      "   0.09190421 -0.36548045 -0.68589797 -1.45638544 -1.15183806  1.35269884\n",
      "  -0.22851331 -1.45638544  1.04248143  0.46886196  1.04248143  0.68589797\n",
      "   0.08170411  0.39645159  0.68589797 -0.14991542 -0.5782186  -1.45638544\n",
      "   1.04248143  1.08362676 -0.24763248 -0.06712059  0.09190421  0.55784431\n",
      "   0.30983715  1.65724622  0.32598698  1.35269884  0.31059209  1.65724622\n",
      "   0.11227851 -0.20126085  0.68589797  0.09190421  1.44168119  1.81863893\n",
      "   0.0467463   0.14844444 -0.83142054  1.38652227  0.09190421 -1.2788429\n",
      "   0.46886196  1.34702881 -0.52687316  0.68589797  0.77340934  1.35269884\n",
      "   0.52687316  0.08170411  1.28028848  0.05241634  0.66552368  1.54224495\n",
      "   0.50268539 -0.53306069  1.04248143 -0.53306069  1.44168119 -0.83142054\n",
      "   0.57966417  0.77907938  0.42370405  0.77340934  0.37568056  1.04248143\n",
      "  -0.99044535  0.09190421 -0.22851331  0.9757411   1.35269884  0.11227851\n",
      "   0.51268639  1.44168119  2.12318631  0.66552368  0.32169471  0.93480205\n",
      "   1.04248143 -1.35015164  1.35269884 -0.14991542  0.97007106  1.28028848\n",
      "  -0.25780108  0.35696372  0.19813801  0.99044535  0.68589797  0.40212163\n",
      "   1.35269884 -1.15183806 -0.52687316  0.0467463   1.65724622  0.93480205\n",
      "   0.77907938  0.9757411  -0.24928435 -1.45638544  1.65724622  0.23742679\n",
      "   0.86239169  1.50842152  1.04248143 -0.12227951  1.54791499  1.44168119\n",
      "  -0.88276598  0.23742679  1.28028848  1.8129689   0.97007106  1.35269884\n",
      "   1.23934943  0.9757411   1.34702881 -0.23742679  0.68589797  0.70666901\n",
      "  -0.4544628   0.77340934  0.03911321  1.35269884  1.65724622  0.14844444\n",
      "  -0.50649887  1.04248143 -0.53306069 -0.06712059 -0.53306069 -0.653878\n",
      "   0.68589797  1.23934943 -1.12126367 -0.36548045 -0.52687316 -0.52687316\n",
      "   0.68589797 -1.2788429   1.23934943  1.23934943 -0.53306069  0.77907938\n",
      "   0.70666901  0.77907938  0.70666901  0.77175748 -0.14424538  1.81863893\n",
      "  -1.45638544  0.46886196  1.44168119  0.14844444 -0.22284327  1.24501947\n",
      "   0.09190421  1.34702881 -0.14991542  0.68589797  1.65724622  0.0467463\n",
      "  -0.52687316  0.31602468  0.68589797  0.23742679  1.28028848  0.19813801\n",
      "   0.09190421  0.77340934  0.97007106  1.08362676  1.65724622  0.68589797\n",
      "  -0.51268639  1.04248143  0.97007106 -0.52120313  0.86806172 -0.22851331\n",
      "   0.77340934  1.35269884  1.65724622 -0.53306069  0.68589797 -1.15183806\n",
      "   0.81290281 -0.52120313 -1.45638544  0.68589797  0.39645159  0.66552368\n",
      "  -0.20126085  1.35269884  1.04248143  0.39645159  0.05097077 -1.45638544\n",
      "   1.04248143  0.42370405  0.77907938  0.46886196 -1.45638544 -0.52687316\n",
      "  -0.52687316 -0.14991542  1.65724622 -0.53306069  0.66552368  0.09190421\n",
      "  -0.83142054 -0.36548045 -0.14424538  1.24501947 -0.53306069  1.65724622\n",
      "   0.08170411  0.62459023  1.2611693   0.11227851 -0.42063936  0.97007106\n",
      "   0.70666901  0.46886196  0.66552368  1.0819749   0.77907938  0.70666901\n",
      "  -0.52687316 -0.52687316 -1.45638544  0.50835543 -0.4544628   0.82290381\n",
      "   1.34702881  1.35269884  0.70666901  1.54791499 -0.53306069 -0.22851331\n",
      "   0.11227851  1.65724622  1.08362676 -0.91728005  0.11227851  0.68589797\n",
      "   1.0819749   1.04248143 -0.22851331 -0.4544628   0.39645159  1.81863893\n",
      "  -0.06712059 -0.52687316 -1.15183806  0.39645159  0.68589797]]\n",
      "total error:  0.24749163879598662\n",
      "D: [[0.00264962 0.00866853 0.00157324 0.00489163 0.00195955 0.0011602\n",
      "  0.00115364 0.00904571 0.00263464 0.00380121 0.00300167 0.002792\n",
      "  0.00085076 0.00967002 0.00138128 0.00263464 0.00141027 0.00224729\n",
      "  0.002792   0.00760406 0.00518379 0.00085076 0.00157324 0.00168426\n",
      "  0.00188368 0.00398821 0.00300167 0.00168183 0.00885975 0.00072396\n",
      "  0.00141027 0.00489163 0.00150983 0.00204734 0.00755715 0.00104002\n",
      "  0.0036608  0.00792177 0.00115364 0.00499231 0.00104002 0.00398821\n",
      "  0.0040703  0.00309609 0.00224729 0.00104002 0.01411812 0.00115364\n",
      "  0.00355057 0.00104002 0.00157324 0.00713123 0.00157324 0.00224729\n",
      "  0.00484199 0.0066331  0.00224729 0.0038409  0.00250278 0.00104002\n",
      "  0.00157324 0.00150983 0.00348333 0.00417244 0.0040703  0.0025543\n",
      "  0.00327325 0.00085076 0.00322081 0.00115364 0.00608735 0.00085076\n",
      "  0.00499231 0.00364866 0.00885975 0.00489163 0.00105542 0.00072396\n",
      "  0.00467565 0.00384655 0.00194294 0.00111527 0.0040703  0.00124207\n",
      "  0.002792   0.0011602  0.00263464 0.00885975 0.00205898 0.00115364\n",
      "  0.00263464 0.00484199 0.00124027 0.00470223 0.00229354 0.00095445\n",
      "  0.00269915 0.00261839 0.00157324 0.00760406 0.00105542 0.00194294\n",
      "  0.00249917 0.00204734 0.00681636 0.00205898 0.00306467 0.00157324\n",
      "  0.00165728 0.00489163 0.00355057 0.00168183 0.00115364 0.00499231\n",
      "  0.00267229 0.00105542 0.00053389 0.00868107 0.00323467 0.00175211\n",
      "  0.00157324 0.00115658 0.00115364 0.0038409  0.01177163 0.00124027\n",
      "  0.00577433 0.00312257 0.00543989 0.00165728 0.00885975 0.00667082\n",
      "  0.00115364 0.00141027 0.00263464 0.00467565 0.00085076 0.00175211\n",
      "  0.00204734 0.00168183 0.00347758 0.00104002 0.00085076 0.00565787\n",
      "  0.00188368 0.00098728 0.01265563 0.00504249 0.00094905 0.00105542\n",
      "  0.00184569 0.00351906 0.00124027 0.00072808 0.00169139 0.00115364\n",
      "  0.01540929 0.00168183 0.0011602  0.00565787 0.00885975 0.00220109\n",
      "  0.0028325  0.00205898 0.00464009 0.00115364 0.00085076 0.00384655\n",
      "  0.00268887 0.00157324 0.00261839 0.00477189 0.00261839 0.00232041\n",
      "  0.00224729 0.0012921  0.00145406 0.00643082 0.00263464 0.00263464\n",
      "  0.00224729 0.00124207 0.0012921  0.0012921  0.00760406 0.00204734\n",
      "  0.00220109 0.00204734 0.00220109 0.00206239 0.00515448 0.00072396\n",
      "  0.00104002 0.002792   0.00105542 0.00517617 0.00357076 0.0012848\n",
      "  0.0040703  0.0011602  0.0038409  0.00885975 0.00085076 0.00425832\n",
      "  0.00263464 0.00325306 0.00885975 0.00565787 0.00124027 0.00543989\n",
      "  0.00489163 0.00205898 0.00169139 0.00150983 0.00085076 0.00224729\n",
      "  0.0074507  0.01265563 0.00169139 0.00264962 0.00187303 0.00560767\n",
      "  0.00205898 0.00115364 0.00085076 0.00261839 0.00224729 0.00141027\n",
      "  0.00197925 0.00751442 0.00104002 0.00224729 0.00300167 0.00229354\n",
      "  0.00364866 0.00115364 0.00157324 0.0066331  0.00469544 0.00104002\n",
      "  0.00157324 0.00681636 0.00204734 0.002792   0.00104002 0.00755715\n",
      "  0.00263464 0.00518379 0.00085076 0.00261839 0.00868107 0.00489163\n",
      "  0.01024758 0.00643082 0.00386274 0.01549691 0.00261839 0.00085076\n",
      "  0.00484199 0.00238937 0.00126422 0.00398821 0.00292994 0.00169139\n",
      "  0.00904571 0.002792   0.00229354 0.00151232 0.00204734 0.00904571\n",
      "  0.00755715 0.00755715 0.00104002 0.0074185  0.0028325  0.01016068\n",
      "  0.0011602  0.00115364 0.00220109 0.00094905 0.00261839 0.00355057\n",
      "  0.00398821 0.00085076 0.00150983 0.00178308 0.00398821 0.00224729\n",
      "  0.00151232 0.00157324 0.00355057 0.0028325  0.0066331  0.00072396\n",
      "  0.00417244 0.00263464 0.00141027 0.00300167 0.00885975]]\n",
      "classEst:  [[-1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1.  1. -1.\n",
      "  -1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.\n",
      "   1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  -1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1.\n",
      "   1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1.\n",
      "  -1.  1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "   1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      "  -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  -1. -1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.\n",
      "   1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.\n",
      "  -1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  -1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.\n",
      "  -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1.\n",
      "  -1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      "   1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.]]\n",
      "aggClassEst:  [[-0.646419    0.53886223  0.91726555  0.21712009 -0.69768794  1.22181293\n",
      "   1.22748297  0.58145314 -0.40165729  0.03508613  0.27123572  0.59407783\n",
      "   1.53203035  0.64819347  1.04739323 -0.40165729 -1.02662219  0.5606821\n",
      "   0.34364609 -0.40784481 -0.02469954  1.53203035  1.1676973   1.0995114\n",
      "   0.73717581  0.23749438  0.52166747  0.85052522  0.5606821   1.69342306\n",
      "  -1.02662219 -0.03331166  0.95841088  0.6538635  -0.40165729 -1.58160132\n",
      "  -0.32315478 -0.69921001  1.22748297  0.23749438 -1.58160132  0.23749438\n",
      "  -0.03331166 -0.49069632 -0.81111385 -1.58160132 -1.27705394  1.22748297\n",
      "  -0.10329743 -1.33116957  0.91726555  0.59407783  0.91726555  0.5606821\n",
      "   0.20691998  0.27123572  0.5606821  -0.02469954 -0.70343447 -1.58160132\n",
      "   1.1676973   0.95841088 -0.37284836 -0.19233647  0.21712009  0.68306018\n",
      "   0.18462128  1.53203035  0.20077111  1.22748297  0.18537621  1.53203035\n",
      "  -0.01293737 -0.32647673  0.5606821  -0.03331166  1.31646531  1.69342306\n",
      "  -0.07846957  0.02322857 -0.70620467  1.2613064   0.21712009 -1.40405878\n",
      "   0.59407783  1.47224468 -0.40165729  0.5606821   0.89862521  1.22748297\n",
      "   0.40165729  0.20691998  1.1550726  -0.07279954  0.79073955  1.41702908\n",
      "   0.62790127 -0.40784481  1.1676973  -0.40784481  1.31646531 -0.95663642\n",
      "   0.4544483   0.90429525  0.29848817  0.89862521  0.50089643  1.1676973\n",
      "  -0.86522948  0.21712009 -0.35372918  0.85052522  1.22748297 -0.01293737\n",
      "   0.38747052  1.56689706  1.99797044  0.54030781  0.44691059  0.80958618\n",
      "   1.1676973  -1.22493577  1.22748297 -0.27513129  0.84485519  1.40550435\n",
      "  -0.38301695  0.48217959  0.07292214  0.86522948  0.5606821   0.5273375\n",
      "   1.22748297 -1.27705394 -0.40165729 -0.07846957  1.53203035  0.80958618\n",
      "   0.90429525  0.85052522 -0.12406847 -1.58160132  1.53203035  0.36264266\n",
      "   0.98760756  1.38320565  0.91726555 -0.24749538  1.42269911  1.31646531\n",
      "  -1.00798185  0.11221091  1.1550726   1.93818477  0.84485519  1.22748297\n",
      "   1.11413356  1.10095697  1.22181293 -0.36264266  0.5606821   0.58145314\n",
      "  -0.57967867  0.64819347  0.16432908  1.22748297  1.53203035  0.27366031\n",
      "  -0.63171474  0.91726555 -0.65827656  0.05809528 -0.65827656 -0.77909388\n",
      "   0.5606821   1.36456531 -1.24647954 -0.24026458 -0.65208904 -0.40165729\n",
      "   0.5606821  -1.40405878  1.11413356  1.36456531 -0.40784481  0.90429525\n",
      "   0.83188488  0.6538635   0.58145314  0.64654161 -0.01902951  1.69342306\n",
      "  -1.33116957  0.34364609  1.31646531  0.02322857 -0.0976274   1.1198036\n",
      "   0.21712009  1.22181293 -0.27513129  0.81111385  1.53203035 -0.07846957\n",
      "  -0.65208904  0.44124055  0.5606821   0.36264266  1.1550726   0.07292214\n",
      "  -0.03331166  0.89862521  0.84485519  1.20884263  1.53203035  0.5606821\n",
      "  -0.63790227  1.1676973   0.84485519 -0.39598725  0.9932776  -0.35372918\n",
      "   0.64819347  1.22748297  1.53203035 -0.65827656  0.5606821  -1.02662219\n",
      "   0.93811868 -0.39598725 -1.58160132  0.5606821   0.52166747  0.79073955\n",
      "  -0.32647673  1.22748297  1.1676973   0.27123572 -0.07424511 -1.33116957\n",
      "   0.91726555  0.54891992  0.6538635   0.59407783 -1.58160132 -0.40165729\n",
      "  -0.65208904 -0.02469954  1.53203035 -0.40784481  0.54030781  0.21712009\n",
      "  -0.95663642 -0.49069632 -0.01902951  1.1198036  -0.65827656  1.53203035\n",
      "   0.20691998  0.49937436  1.13595343 -0.01293737 -0.29542349  0.84485519\n",
      "   0.83188488  0.34364609  0.54030781  0.95675902  0.90429525  0.83188488\n",
      "  -0.65208904 -0.40165729 -1.58160132  0.38313956 -0.32924692  0.69768794\n",
      "   1.47224468  1.22748297  0.83188488  1.42269911 -0.65827656 -0.35372918\n",
      "  -0.01293737  1.53203035  0.95841088 -1.04249592  0.23749438  0.5606821\n",
      "   1.20719077  0.91726555 -0.10329743 -0.57967867  0.27123572  1.69342306\n",
      "   0.05809528 -0.65208904 -1.02662219  0.27123572  0.5606821 ]]\n",
      "total error:  0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "datArr,labelArr = loadDataSet('D:/10-Book/MachineLearninginaction/Ch07/horseColicTraining2.txt')\n",
    "classifierArray = adaboostTrainDS(datArr,labelArr,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]]\n",
      "[[ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]]\n",
      "[[ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [-0.1376298 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.43598966]\n",
      " [ 0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]]\n",
      "[[ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [-0.37059985]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [-0.20301961]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-0.66895971]\n",
      " [-0.82798452]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-1.29392461]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 1.29392461]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [ 0.25436505]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [ 0.09534024]\n",
      " [-0.37059985]\n",
      " [ 0.72030514]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.66895971]\n",
      " [-0.09534024]\n",
      " [ 0.72030514]\n",
      " [-1.29392461]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]]\n",
      "[[ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [-0.47092125]\n",
      " [ 0.62994605]\n",
      " [ 0.91834361]\n",
      " [-0.17256139]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.17256139]\n",
      " [-0.00498115]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.45240351]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-0.47092125]\n",
      " [-0.62994605]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-1.09588615]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 1.49196307]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.05632659]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.56863831]\n",
      " [ 0.45240351]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [ 0.2933787 ]\n",
      " [-0.17256139]\n",
      " [ 0.91834361]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [ 0.86699817]\n",
      " [ 0.10269822]\n",
      " [ 0.91834361]\n",
      " [-1.09588615]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]]\n",
      "[[ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.75711718]\n",
      " [-0.65940012]\n",
      " [ 0.44146718]\n",
      " [ 0.72986473]\n",
      " [-0.36104026]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.36104026]\n",
      " [ 0.18349772]\n",
      " [-0.65940012]\n",
      " [ 0.24480546]\n",
      " [ 0.64088239]\n",
      " [ 0.64088239]\n",
      " [ 0.8375441 ]\n",
      " [-0.28244237]\n",
      " [-0.65940012]\n",
      " [-0.44146718]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [-0.65940012]\n",
      " [-1.28436502]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 1.68044194]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 0.24480546]\n",
      " [-0.13215228]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.72986473]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.38015944]\n",
      " [ 0.26392464]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [ 0.38015944]\n",
      " [ 0.10489983]\n",
      " [-0.36104026]\n",
      " [ 1.10682248]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [ 0.6785193 ]\n",
      " [-0.08578066]\n",
      " [ 1.10682248]\n",
      " [-0.90740727]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 1.21450185]\n",
      " [ 1.10682248]\n",
      " [ 0.8375441 ]\n",
      " [ 0.26392464]]\n",
      "[[ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [ 0.60484349]\n",
      " [-0.81167381]\n",
      " [ 0.28919349]\n",
      " [ 0.88213842]\n",
      " [-0.20876657]\n",
      " [ 0.97069862]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.20876657]\n",
      " [ 0.03122403]\n",
      " [-0.50712643]\n",
      " [ 0.39707915]\n",
      " [ 0.79315608]\n",
      " [ 0.79315608]\n",
      " [ 0.68527041]\n",
      " [-0.43471606]\n",
      " [-0.81167381]\n",
      " [-0.59374087]\n",
      " [-0.50712643]\n",
      " [ 0.98981779]\n",
      " [-0.50712643]\n",
      " [-1.43663871]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 1.83271563]\n",
      " [ 0.4886087 ]\n",
      " [ 1.06222816]\n",
      " [ 0.4886087 ]\n",
      " [ 1.36677554]\n",
      " [-0.20876657]\n",
      " [-0.43471606]\n",
      " [ 0.09253177]\n",
      " [-0.28442597]\n",
      " [ 0.68527041]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 0.88213842]\n",
      " [ 0.98981779]\n",
      " [ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [-0.51331395]\n",
      " [-0.53243313]\n",
      " [ 0.11165095]\n",
      " [-0.50712643]\n",
      " [ 0.68527041]\n",
      " [ 0.22788575]\n",
      " [-0.04737386]\n",
      " [-0.51331395]\n",
      " [ 0.95454879]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 0.66615124]\n",
      " [ 0.98981779]\n",
      " [ 0.52624561]\n",
      " [-0.23805435]\n",
      " [ 1.25909617]\n",
      " [-1.05968096]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.43471606]\n",
      " [ 1.06222816]\n",
      " [ 0.95454879]\n",
      " [ 0.68527041]\n",
      " [ 0.11165095]]\n",
      "[[ 1.21166683]\n",
      " [ 1.21733687]\n",
      " [ 0.44973479]\n",
      " [-0.96678252]\n",
      " [ 0.13408478]\n",
      " [ 1.03724713]\n",
      " [-0.36387528]\n",
      " [ 0.81558991]\n",
      " [ 0.83470909]\n",
      " [-0.66842266]\n",
      " [-0.36387528]\n",
      " [-0.12388468]\n",
      " [-0.66223514]\n",
      " [ 0.24197045]\n",
      " [ 0.63804737]\n",
      " [ 0.94826478]\n",
      " [ 0.84037912]\n",
      " [-0.58982477]\n",
      " [-0.96678252]\n",
      " [-0.74884958]\n",
      " [-0.66223514]\n",
      " [ 0.83470909]\n",
      " [-0.66223514]\n",
      " [-1.59174742]\n",
      " [ 1.68327696]\n",
      " [ 0.90711945]\n",
      " [ 1.67760692]\n",
      " [ 0.33349999]\n",
      " [ 1.21733687]\n",
      " [ 0.6437174 ]\n",
      " [ 1.52188425]\n",
      " [-0.36387528]\n",
      " [-0.58982477]\n",
      " [-0.06257693]\n",
      " [-0.43953468]\n",
      " [ 0.84037912]\n",
      " [ 1.21733687]\n",
      " [ 1.21733687]\n",
      " [ 0.90711945]\n",
      " [ 0.72702971]\n",
      " [ 0.83470909]\n",
      " [ 1.21166683]\n",
      " [ 0.90711945]\n",
      " [-0.66842266]\n",
      " [-0.68754184]\n",
      " [-0.04345776]\n",
      " [-0.66223514]\n",
      " [ 0.84037912]\n",
      " [ 0.38299446]\n",
      " [-0.20248257]\n",
      " [-0.66842266]\n",
      " [ 0.79944008]\n",
      " [ 1.37305954]\n",
      " [ 1.21733687]\n",
      " [ 0.82125995]\n",
      " [ 1.1449265 ]\n",
      " [ 0.68135431]\n",
      " [-0.39316305]\n",
      " [ 1.10398746]\n",
      " [-1.21478967]\n",
      " [ 1.1449265 ]\n",
      " [-0.66842266]\n",
      " [-0.58982477]\n",
      " [ 1.21733687]\n",
      " [ 0.79944008]\n",
      " [ 0.53016171]\n",
      " [ 0.26675966]]\n",
      "[[ 1.07630486]\n",
      " [ 1.0819749 ]\n",
      " [ 0.31437281]\n",
      " [-0.83142054]\n",
      " [ 0.26944676]\n",
      " [ 1.1726091 ]\n",
      " [-0.22851331]\n",
      " [ 0.95095188]\n",
      " [ 0.97007106]\n",
      " [-0.53306069]\n",
      " [-0.22851331]\n",
      " [-0.25924665]\n",
      " [-0.52687316]\n",
      " [ 0.37733242]\n",
      " [ 0.77340934]\n",
      " [ 1.08362676]\n",
      " [ 0.9757411 ]\n",
      " [-0.4544628 ]\n",
      " [-0.83142054]\n",
      " [-0.88421155]\n",
      " [-0.52687316]\n",
      " [ 0.97007106]\n",
      " [-0.52687316]\n",
      " [-1.45638544]\n",
      " [ 1.81863893]\n",
      " [ 1.04248143]\n",
      " [ 1.8129689 ]\n",
      " [ 0.46886196]\n",
      " [ 1.35269884]\n",
      " [ 0.50835543]\n",
      " [ 1.65724622]\n",
      " [-0.22851331]\n",
      " [-0.4544628 ]\n",
      " [-0.19793891]\n",
      " [-0.30417271]\n",
      " [ 0.9757411 ]\n",
      " [ 1.35269884]\n",
      " [ 1.35269884]\n",
      " [ 1.04248143]\n",
      " [ 0.86239169]\n",
      " [ 0.97007106]\n",
      " [ 1.34702881]\n",
      " [ 1.04248143]\n",
      " [-0.53306069]\n",
      " [-0.55217986]\n",
      " [ 0.09190421]\n",
      " [-0.52687316]\n",
      " [ 0.9757411 ]\n",
      " [ 0.51835643]\n",
      " [-0.06712059]\n",
      " [-0.53306069]\n",
      " [ 0.93480205]\n",
      " [ 1.50842152]\n",
      " [ 1.35269884]\n",
      " [ 0.68589797]\n",
      " [ 1.28028848]\n",
      " [ 0.81671629]\n",
      " [-0.25780108]\n",
      " [ 1.23934943]\n",
      " [-1.0794277 ]\n",
      " [ 1.28028848]\n",
      " [-0.53306069]\n",
      " [-0.4544628 ]\n",
      " [ 1.35269884]\n",
      " [ 0.93480205]\n",
      " [ 0.66552368]\n",
      " [ 0.40212163]]\n",
      "[[ 0.95108899]\n",
      " [ 1.20719077]\n",
      " [ 0.18915694]\n",
      " [-0.95663642]\n",
      " [ 0.14423088]\n",
      " [ 1.29782498]\n",
      " [-0.10329743]\n",
      " [ 0.82573601]\n",
      " [ 1.09528693]\n",
      " [-0.65827656]\n",
      " [-0.35372918]\n",
      " [-0.38446252]\n",
      " [-0.40165729]\n",
      " [ 0.50254829]\n",
      " [ 0.64819347]\n",
      " [ 1.20884263]\n",
      " [ 0.85052522]\n",
      " [-0.57967867]\n",
      " [-0.70620467]\n",
      " [-0.75899568]\n",
      " [-0.65208904]\n",
      " [ 1.09528693]\n",
      " [-0.40165729]\n",
      " [-1.33116957]\n",
      " [ 1.69342306]\n",
      " [ 1.1676973 ]\n",
      " [ 1.68775303]\n",
      " [ 0.34364609]\n",
      " [ 1.22748297]\n",
      " [ 0.38313956]\n",
      " [ 1.53203035]\n",
      " [-0.35372918]\n",
      " [-0.57967867]\n",
      " [-0.32315478]\n",
      " [-0.17895684]\n",
      " [ 0.85052522]\n",
      " [ 1.22748297]\n",
      " [ 1.22748297]\n",
      " [ 0.91726555]\n",
      " [ 0.98760756]\n",
      " [ 0.84485519]\n",
      " [ 1.47224468]\n",
      " [ 0.91726555]\n",
      " [-0.65827656]\n",
      " [-0.67739574]\n",
      " [ 0.21712009]\n",
      " [-0.40165729]\n",
      " [ 0.85052522]\n",
      " [ 0.39314056]\n",
      " [ 0.05809528]\n",
      " [-0.40784481]\n",
      " [ 0.80958618]\n",
      " [ 1.63363739]\n",
      " [ 1.22748297]\n",
      " [ 0.81111385]\n",
      " [ 1.1550726 ]\n",
      " [ 0.69150041]\n",
      " [-0.38301695]\n",
      " [ 1.11413356]\n",
      " [-1.20464357]\n",
      " [ 1.1550726 ]\n",
      " [-0.40784481]\n",
      " [-0.32924692]\n",
      " [ 1.47791472]\n",
      " [ 0.80958618]\n",
      " [ 0.54030781]\n",
      " [ 0.5273375 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testArr,testLabelArr = loadDataSet('D:/10-Book/MachineLearninginaction/Ch07/horseColicTest2.txt')\n",
    "prediction10 = adaClassify(testArr,classifierArray)\n",
    "errArr = np.mat(np.ones((67,1)))\n",
    "errArr[prediction10 != np.mat(testLabelArr).T].sum()\n",
    "\"\"\"得到错误率，只需将上述错分样例的个数除以67即可\n",
    "\n",
    "该数据集是个难数据集，通常情况下，AdaBoost会达到一个稳定的测试错误率，而并不会随分类器数据的增多而提高\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多人认为AdaBoost和SVM是监督机器学习中最强大的两种方法。实际上这两者之间拥有不少相似之处。我们可以把弱分类器想象成SVM中的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法。而他们的不同就在于其定义的间隔计算方式有所不同，因此导致的结果也不同。特别是在高维空间下，这两者之间的差异就会更加明显。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboostTrainDS(dataArr,classLabels,numIt=40):#函数名称尾部的DS代表单层决策树\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    #D是一个概率分布向量，因此所有的元素之和为1.所以一开始所有的元素初始化为1/m\n",
    "    D = np.mat(np.ones((m,1))/m)#建立一个列向量，包含了每个数据点的权重，概率分布向量\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))#列向量 用于记录每个数据点的类别估计累计值\n",
    "    for i in range(numIt):#核心部分\n",
    "        #建立一个单层决策树，输入为D，返回利用D得到的具有最小错误率的单层决策树，最小错误率以及估计的类别向量\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)\n",
    "        #print(\"D:\", D.T)\n",
    "        alpha = np.float(0.5*np.log((1.0-error)/max(error,1e-16)))#alpha用于告诉总分类器本次DS输出结果的权重\n",
    "        bestStump['alpha'] = alpha#加入字典中\n",
    "        weakClassArr.append(bestStump)#字典添加到列表，字典包含了分类所需要的所有信息\n",
    "        #print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1*alpha*np.mat(classLabels).T,classEst)#计算下一次迭代的新权重\n",
    "        D = np.multiply(D,np.exp(expon))\n",
    "        D = D/D.sum()\n",
    "        aggClassEst += alpha*classEst\n",
    "        #print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T,np.ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr,aggClassEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ROC曲线绘制及AUC计算函数\n",
    "AUC：曲线下的面积。AUC给出的是分类器的平均性能值，当然不能完全代替对整条曲线的观察，\n",
    "一个完美分类器的AUC是1.0，而随机猜测的AUC为0.5\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "def plotROC(predStrengths,classLabels):\n",
    "    cur = (1.0,1.0)\n",
    "    ySum = 0.0\n",
    "    numPosClass = sum(np.array(classLabels)==1.0)\n",
    "    yStep = 1/np.float(numPosClass)\n",
    "    xStep = 1/np.float(len(classLabels)-numPosClass)\n",
    "    sortedIndicies = predStrengths.argsort()\n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index] == 1.0:\n",
    "            deIX = 0.0\n",
    "            deIY = yStep\n",
    "        else:\n",
    "            deIX = xStep\n",
    "            deIY = 0\n",
    "            ySum += cur[1]\n",
    "        ax.plot([cur[0],cur[0]-deIX],[cur[1],cur[1]-deIY],c='b')\n",
    "        cur = (cur[0]-deIX,cur[1]-deIY)\n",
    "    ax.plot([0,1],[0,1],'b--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC cueve for AdaBoost Horse Coloc Detection System')\n",
    "    ax.axis([0,1,0,1])\n",
    "    plt.show()\n",
    "    print(\"AUC为：\",ySum*xStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2842809364548495\n",
      "total error:  0.2842809364548495\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.25418060200668896\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.22073578595317725\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.23076923076923078\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC为： 0.8582969635063604\n"
     ]
    }
   ],
   "source": [
    "datArr,labelArr = loadDataSet('D:/10-Book/MachineLearninginaction/Ch07/horseColicTraining2.txt')\n",
    "classifierArray,aggClassEst = adaboostTrainDS(datArr,labelArr,10)\n",
    "plotROC(aggClassEst.T,labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2 基于代价函数的分类器决策控制\n",
    "\n",
    "在分类算法中，有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量D。在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。在SVM中，可以在代价函数中对于不同的类别选择不同的参数c。\n",
    "\n",
    "### 7.7.3 处理非均衡问题的数据抽样方法\n",
    "另一种针对非均衡问题调节分类器的方法，就是对分类器的训练数据进行改造。这可以通过欠抽样或者过抽样来实现。过抽样意味着复制样例，欠抽样意味着删除样例。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
